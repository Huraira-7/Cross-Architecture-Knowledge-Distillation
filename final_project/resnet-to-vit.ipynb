{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"collapsed_sections":["yeumJlGyU55B","i3nZOOA2YuE_"],"machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10268581,"sourceType":"datasetVersion","datasetId":6353085},{"sourceId":10271970,"sourceType":"datasetVersion","datasetId":6355573},{"sourceId":10271980,"sourceType":"datasetVersion","datasetId":6355583},{"sourceId":10272005,"sourceType":"datasetVersion","datasetId":6355605},{"sourceId":10272026,"sourceType":"datasetVersion","datasetId":6355626},{"sourceId":10279765,"sourceType":"datasetVersion","datasetId":6361123},{"sourceId":10296308,"sourceType":"datasetVersion","datasetId":6372764},{"sourceId":10296360,"sourceType":"datasetVersion","datasetId":6372806},{"sourceId":10296393,"sourceType":"datasetVersion","datasetId":6372837},{"sourceId":10296443,"sourceType":"datasetVersion","datasetId":6372878},{"sourceId":10296481,"sourceType":"datasetVersion","datasetId":6372911},{"sourceId":10296936,"sourceType":"datasetVersion","datasetId":6373249}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading Libraries, Models, Datasets","metadata":{"id":"1ujhz5OIShCO"}},{"cell_type":"code","source":"!pip install timm","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJIyu_3ESWsB","outputId":"833e8a70-9a4b-40cb-e499-83d926322fe9","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:23:52.558380Z","iopub.execute_input":"2024-12-25T17:23:52.558694Z","iopub.status.idle":"2024-12-25T17:23:58.512550Z","shell.execute_reply.started":"2024-12-25T17:23:52.558668Z","shell.execute_reply":"2024-12-25T17:23:58.511580Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport math\nimport torch\nimport random\nimport shutil\nimport zipfile\nimport numpy as np\nfrom tqdm import tqdm\nfrom copy import deepcopy\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom google.colab import drive,files\n\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom timm import create_model\nimport torch.nn.functional as F\nfrom torchvision.models import resnet50\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CIFAR10, ImageFolder\nfrom torch.utils.data import TensorDataset, DataLoader","metadata":{"id":"emHMoqI5SeUD","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:23:58.513788Z","iopub.execute_input":"2024-12-25T17:23:58.514113Z","iopub.status.idle":"2024-12-25T17:24:08.845161Z","shell.execute_reply.started":"2024-12-25T17:23:58.514074Z","shell.execute_reply":"2024-12-25T17:24:08.844448Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef create_vit_model(num_classes=10):\n    \"\"\"Create a ViT model with a dynamic number of output classes.\"\"\"\n    model = create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n\n    #Freeze all layers in the VIT model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    model.head = torch.nn.Linear(model.head.in_features, num_classes)  # Replace the classifier head\n\n    # Get features model (backbone)\n    # print(nn.Sequential(*list(model.children())))\n    # print(\"--------------------------------(--------------------------------------)------------\")\n    vit_features_model = nn.Sequential(*list(model.children())[:-1])  # Exclude the classifier head\n    # print(vit_features_model)\n    # print(\"-----------------------------------(--------------------------------------)---------\")\n\n    return model, vit_features_model\n\ndef create_resnet_model(num_classes=10):\n    \"\"\"Create a RESNET model with a dynamic number of output classes.\"\"\"\n    model = resnet50(pretrained=True)\n\n    # Freeze all layers in the RESNET model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    num_features = model.fc.in_features       # Get the number of input features of the last layer\n    model.fc = nn.Linear(num_features, num_classes)      # Replace the last fully connected layer with a new one with `num_classes` outputs\n\n    # Get features model (backbone)\n    # print(nn.Sequential(*list(model.children())))\n    # print(\"-----------------------(--------------------------------------)---------------\")\n\n    features_model = nn.Sequential(*list(model.children())[:-2])  # Extract all layers except the last fully connected layer and the global adaptive average pool 2d layer\n    # print(features_model)\n    # print(\"----------------------------(--------------------------------------)----------\")\n    return model, features_model\n\n\n# def get_model(name):\n#   if name == 'student':\n#     resnet_model, resnet_features_model = create_resnet_model()\n#     return resnet_model, resnet_features_model\n#   elif name == 'teacher':\n#     vit_model, vit_features_model = create_vit_model()\n#     return vit_model, vit_features_model\n#   else:\n#     raise ValueError(f\"Unsupported model name: {name}\")\n\ndef get_model(name):\n  if name == 'student':\n    # Student is now ViT\n    vit_model, vit_features_model = create_vit_model()\n    return vit_model, vit_features_model\n  \n  elif name == 'teacher':\n    # Teacher is now ResNet\n    resnet_model, resnet_features_model = create_resnet_model()\n    return resnet_model, resnet_features_model\n  \n  else:\n    raise ValueError(f\"Unsupported model name: {name}\")\n\n\n\ndef finetune_model(model, train_loader, num_epochs=10, alpha=1e-3):\n    \"\"\"Fine-tune the model on the training data.\"\"\"\n    print(\"Finetuning the model on this data\")\n    model.train()\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=alpha)\n    num_batches = len(train_loader)\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        # running_accuracy = 0.0\n        correct = 0\n        total = 0\n        for batch_index, (inputs, labels) in enumerate(train_loader):\n\n            batch_size = inputs.size(0)\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            # Calculate accuracy\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            if batch_index % 300 == 0 or batch_index == 750:\n                print(f'Batch {batch_index + 1}/{num_batches} - Loss: {loss.item():.4f} Accuracy: {(correct/total) * 100:.2f}%')\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy =  correct / total\n        print(f'------------------------->Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f} Accuracy : {epoch_accuracy * 100:.2f}%')\n\n    return epoch_loss, epoch_accuracy\n\ndef evaluate_model(model, data_loader,device):\n    \"\"\"Evaluate the models using a basic testing loop\"\"\"\n    print(\"Evaluating the model\")\n    model.to(device)\n    model.eval()\n    correct = 0\n    total = 0\n    num_batches = len(data_loader)  # Get total number of batches\n\n    with torch.no_grad():\n        for batch_index, (images, labels) in enumerate(data_loader):\n\n            batch_size = images.size(0)\n            if batch_size < 32:\n              print(f\"skipping last batch of size {batch_size} --- gives shape error\")\n              break\n\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            # Print progress every 10 batches (you can adjust this as needed)\n            if batch_index % 10 == 0:\n                print(f'Batch {batch_index + 1}/{num_batches} - Accuracy: { (correct/total) * 100:.2f}%')\n\n    accuracy = correct / total\n    print(f'Final Accuracy: {accuracy * 100:.2f}%')\n    return accuracy","metadata":{"id":"M_cW8c5YStQ4","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:24:13.805860Z","iopub.execute_input":"2024-12-25T17:24:13.806190Z","iopub.status.idle":"2024-12-25T17:24:14.046581Z","shell.execute_reply.started":"2024-12-25T17:24:13.806155Z","shell.execute_reply":"2024-12-25T17:24:14.045600Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # works for all models\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.2435, 0.2616))  # CIFAR-10 mean and std\n])\n\nmean, std = (0.4914, 0.4822, 0.4465), (0.247, 0.2435, 0.2616)\n\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# print(f'Total number of images in CIFAR 10 test_loader: {len(test_dataset)}')\n\ncifar10_classes = [ 'airplane', 'automobile',  'ship', 'truck' ,'bird', 'cat', 'deer', 'dog', 'frog', 'horse']\nnum_classes = len(cifar10_classes)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgBgJjVeUdNW","outputId":"9210b468-e563-4b31-9967-65d50c9091e8","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:24:08.846282Z","iopub.execute_input":"2024-12-25T17:24:08.846528Z","iopub.status.idle":"2024-12-25T17:24:13.804438Z","shell.execute_reply.started":"2024-12-25T17:24:08.846508Z","shell.execute_reply":"2024-12-25T17:24:13.803710Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:01<00:00, 103265683.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Knowledge Distillation Utils","metadata":{"id":"PFQ4JBiDWTId"}},{"cell_type":"code","source":"def get_features_and_logits(model,feature_model,inputs,student=True, crd=False):\n    #will return the same logits as long as model.eval()  is set (i.e. no learning is happening hence same LOGITs)\n    if not student:\n      model.eval()\n      feature_model.eval()\n      # vit_transform = transforms.Compose([\n      #     transforms.Resize((224, 224)),\n          # transforms.ToTensor()\n      # ])\n      # inputs = vit_transform(inputs)\n      features = feature_model(inputs)\n      # if crd:\n      return features,None    #teacher logits NOT required in CRD / Hint\n      # logits = model.classifier(features.view(features.size(0), -1))  #to save on time (as teacher doesnt need to LEARN)\n    else:\n      model.train()\n      features = feature_model(inputs)\n      logits = model(inputs) #actual student model does need to LEARN hence repeating it twice\n\n    # print(\"feature shape\",features.shape,\"logits shape\",logits.shape)\n    return features, logits\n\ndef get_channel_num(model,name):\n\n\n    if name == 'resnet':\n      return 2048\n      channel_nums = []\n      for layer in model.modules():        # find all Conv2d layers in the model\n          if isinstance(layer, nn.Conv2d):\n              channel_nums.append(layer.out_channels)\n\n      # Check if there are enough Conv2d layers\n      if len(channel_nums) < 2:\n          raise ValueError(\"Model has fewer than two Conv2d layers.\")\n\n      # print(channel_nums,name)\n      return channel_nums[-2]      # Return the output channels of the second-to-last Conv2d layer\n\n    elif name == 'vit':\n        return 150528\n        # For ViT, return the embedding dimension (projection size)\n        for layer in model.modules():\n            # print(layer)\n            if isinstance(layer, nn.Linear):\n                # Typically, the first Linear layer defines the embedding size\n                return layer.out_features\n\n        raise ValueError(\"No Linear layers found in the ViT model.\")\n\n    else:\n        raise ValueError(f\"Unsupported model type: {name}\")\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10FvPJ4LWUJh","outputId":"dd7e3035-d074-4102-e59e-ed2206ed470c","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:56:01.351592Z","iopub.execute_input":"2024-12-25T15:56:01.351845Z","iopub.status.idle":"2024-12-25T15:56:01.358036Z","shell.execute_reply.started":"2024-12-25T15:56:01.351825Z","shell.execute_reply":"2024-12-25T15:56:01.357113Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"student_model, features_student = get_model('student')\nteacher_model, features_teacher = get_model('teacher')\nx,y = get_channel_num(student_model,\"vit\") , get_channel_num(teacher_model,\"resnet\")\nx,\"resnet\", y, \"vit\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_student(student, teacher, train_loader, kd, num_epochs=10, alpha=1e-3,  features_student=None, features_teacher=None):\n    \"\"\"Train the model on the training data.\"\"\"\n    student.train()\n    teacher.eval()\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(student.parameters(), lr=alpha)\n    num_batches = len(train_loader)\n\n    if kd is None:\n      print(\"Training the student on this data independently\")\n    else:\n      print(\"Distilling the knowledge and training the student on this data\",kd.__class__.__name__)\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        running_accuracy = 0.0\n        correct = 0\n        total = 0\n        for batch_index, (inputs, labels) in enumerate(train_loader):\n\n            batch_size = inputs.size(0)\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            if kd is None:      #independent student\n              outputs = student(inputs)\n              distillation_loss = 0.0\n            else:\n              if kd.__class__.__name__ in [ 'LogitMatching' , 'LabelSmoothingRegularization', 'DecoupledLogitMatching'] :\n\n                  #get teacher's logits and student's logits\n                  with torch.no_grad():\n                    teacher_outputs = teacher(inputs)\n                  outputs = student(inputs)\n\n                  if kd.__class__.__name__ == 'LogitMatching':\n                    distillation_loss = kd(outputs, teacher_outputs)\n                  elif kd.__class__.__name__ == 'LabelSmoothingRegularization':\n                   distillation_loss = kd(outputs,labels)\n                  else:     #DecoupledLogitMatching\n                    distillation_loss = kd(outputs, teacher_outputs, labels)\n\n              elif kd.__class__.__name__ == 'FeatureMatching':\n                  #get features of student , teacher and logits of student\n                  feat_s, outputs = get_features_and_logits(student,features_student,inputs)\n                  feat_t, _ = get_features_and_logits(teacher,features_teacher,inputs,student=False)\n                  distillation_loss = kd(feat_s, feat_t)\n\n            classification_loss = criterion(outputs, labels)\n            total_loss = classification_loss + distillation_loss\n            total_loss.backward()\n            optimizer.step()\n\n            running_loss += total_loss.item()\n            # Calculate accuracy\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            if batch_index % 200 == 0 or batch_index == 750:\n                print(f'Batch {batch_index + 1}/{num_batches} - Loss: {total_loss.item():.4f} Accuracy: {(correct/total) * 100:.2f}%')\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy = correct/total\n        print(f'------------------------->Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f} Accuracy : {epoch_accuracy * 100:.2f}%')\n\n    return epoch_loss, epoch_accuracy","metadata":{"id":"mIdfk_iWWZDM","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:56:07.479251Z","iopub.execute_input":"2024-12-25T15:56:07.479566Z","iopub.status.idle":"2024-12-25T15:56:07.488796Z","shell.execute_reply.started":"2024-12-25T15:56:07.479543Z","shell.execute_reply":"2024-12-25T15:56:07.487910Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def plot_accuracies(accuracies, model_labels, title='Model Accuracies'):\n    \"\"\"\n    Plots the accuracies of different models.\n\n    Parameters:\n    accuracies (list of float): A list of accuracy values for each model.\n    model_labels (list of str): A list of labels corresponding to each model.\n\n    \"\"\"\n    # Ensure the inputs are valid\n    if len(accuracies) != len(model_labels):\n        raise ValueError(\"The number of accuracies must match the number of model labels.\")\n\n    # Create the plot\n    plt.figure(figsize=(8, 6))\n    plt.bar(model_labels, accuracies)\n\n    # Adding title and labels\n    plt.title(title)\n    plt.xlabel('Models')\n    plt.ylabel('Accuracy')\n\n    # Optionally, add accuracy values on top of the bars\n    for i, acc in enumerate(accuracies):\n        plt.text(i, acc, f'{acc:.2f}', ha='center', va='bottom')\n\n    # Show the plot\n    plt.xticks(rotation=45)  # Rotate x labels for better visibility\n    plt.tight_layout()\n    plt.show()\n","metadata":{"id":"9h4FEDI6WeRL","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:56:12.218720Z","iopub.execute_input":"2024-12-25T15:56:12.219035Z","iopub.status.idle":"2024-12-25T15:56:12.224460Z","shell.execute_reply.started":"2024-12-25T15:56:12.219007Z","shell.execute_reply":"2024-12-25T15:56:12.223384Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class CIFAR10IdxSample(CIFAR10):          #loaded for Knowledge Distillation with CRD\n\tdef __init__(self, root, train=True, transform=None, target_transform=None, download=False, n=4096, mode='exact', percent=1.0):\n\t\tsuper().__init__(root=root, train=train, download=download,transform=transform, target_transform=target_transform)\n\t\tself.n = n\n\t\tself.mode = mode\n\n\t\tnum_classes = 10\n\t\tnum_samples = len(self.data)\n\t\tlabels = self.targets\n\n\t\tself.cls_positive = [[] for _ in range(num_classes)]\n\t\tfor i in range(num_samples):\n\t\t\tself.cls_positive[labels[i]].append(i)\n\n\t\tself.cls_negative = [[] for _ in range(num_classes)]\n\t\tfor i in range(num_classes):\n\t\t\tfor j in range(num_classes):\n\t\t\t\tif j == i:\n\t\t\t\t\tcontinue\n\t\t\t\tself.cls_negative[i].extend(self.cls_positive[j])\n\n\t\tself.cls_positive = [np.asarray(self.cls_positive[i]) for i in range(num_classes)]\n\t\tself.cls_negative = [np.asarray(self.cls_negative[i]) for i in range(num_classes)]\n\n\t\tif 0 < percent < 1:\n\t\t\tnum = int(len(self.cls_negative[0]) * percent)\n\t\t\tself.cls_negative = [np.random.permutation(self.cls_negative[i])[0:num]\n\t\t\t\t\t\t\t\t for i in range(num_classes)]\n\n\t\tself.cls_positive = np.asarray(self.cls_positive)\n\t\tself.cls_negative = np.asarray(self.cls_negative)\n\n\tdef __getitem__(self, index):\n\t\timg, target = self.data[index], self.targets[index]\n\n\t\timg = Image.fromarray(img)\n\t\tif self.transform is not None:\n\t\t\timg = self.transform(img)\n\n\t\tif self.target_transform is not None:\n\t\t\ttarget = self.target_transform(target)\n\n\t\tif self.mode == 'exact':\n\t\t\tpos_idx = index\n\t\telif self.mode == 'relax':\n\t\t\tpos_idx = np.random.choice(self.cls_positive[target], 1)[0]\n\t\telse:\n\t\t\traise NotImplementedError(self.mode)\n\t\treplace = True if self.n > len(self.cls_negative[target]) else False\n\t\tneg_idx = np.random.choice(self.cls_negative[target], self.n, replace=replace)\n\t\tsample_idx = np.hstack((np.asarray([pos_idx]), neg_idx))\n\n\t\treturn img, target, index, sample_idx","metadata":{"id":"d5O7CQORWgv8","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:57:04.269005Z","iopub.execute_input":"2024-12-25T15:57:04.269363Z","iopub.status.idle":"2024-12-25T15:57:04.279005Z","shell.execute_reply.started":"2024-12-25T15:57:04.269332Z","shell.execute_reply":"2024-12-25T15:57:04.278021Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Logit Matching and Contrastive","metadata":{"id":"i3nZOOA2YuE_"}},{"cell_type":"code","source":"class LogitMatching(nn.Module):  #smoothes teacher logits as well (going from Peaky dist to Uniform dist by increasing Temperature T)\n\tdef __init__(self, T=3.0):\n\t\tsuper(LogitMatching, self).__init__()\n\t\tself.T = T      #Temperature for Soft Targets ... lower the T, less the smoothing, less the error\n\n\tdef forward(self, out_s, out_t):\n    # loss = F.mse_loss(out_s, out_t)\n\t\tloss = self.T * self.T * F.kl_div(\n                                        F.log_softmax(out_s/self.T, dim=1),\n                                        F.softmax(out_t/self.T, dim=1),\n                                        reduction='batchmean'\n          )  #cross entropy loss of softened probabilities\n\n\t\treturn loss\n\nclass CRD(nn.Module):\n\t'''\n\tContrastive Representation Distillation\n\thttps://openreview.net/pdf?id=SkgpBJrtvS\n\n\tincludes two symmetric parts:\n\t(a) using teacher as anchor, choose positive and negatives over the student side\n\t(b) using student as anchor, choose positive and negatives over the teacher side\n\n\tArgs:\n\t\ts_dim: the dimension of student's feature\n\t\tt_dim: the dimension of teacher's feature\n\t\tfeat_dim: the dimension of the projection space\n\t\tnce_n: number of negatives paired with each positive\n\t\tnce_t: the temperature\n\t\tnce_mom: the momentum for updating the memory buffer\n\t\tn_data: the number of samples in the training set, which is the M in Eq.(19)\n\t'''\n\tdef __init__(self, s_dim, t_dim, n_data, feat_dim=128, nce_n=4096, nce_t=0.1, nce_mom=0.5):\n\t\tsuper(CRD, self).__init__()\n\t\tself.embed_s = Embed(s_dim, feat_dim)\n\t\tself.embed_t = Embed(t_dim, feat_dim)\n\t\tself.contrast = ContrastMemory(feat_dim, n_data, nce_n, nce_t, nce_mom)\n\t\tself.criterion_s = ContrastLoss(n_data)\n\t\tself.criterion_t = ContrastLoss(n_data)\n\n\tdef forward(self, feat_s, feat_t, idx, sample_idx):\n\t\tfeat_s = self.embed_s(feat_s)\n\t\tfeat_t = self.embed_t(feat_t)\n\t\tout_s, out_t = self.contrast(feat_s, feat_t, idx, sample_idx)\n\t\tloss_s = self.criterion_s(out_s)\n\t\tloss_t = self.criterion_t(out_t)\n\t\tloss = loss_s + loss_t\n\n\t\treturn loss\n\n\nclass Embed(nn.Module):\n\tdef __init__(self, in_dim, out_dim):\n\t\tsuper(Embed, self).__init__()\n\t\tself.linear = nn.Linear(in_dim, out_dim)\n\n\tdef forward(self, x):\n\t\tx = x.view(x.size(0), -1).to(self.linear.weight.device)\n\t \t# x = x.view(x.size(0), -1).to(self.linear.weight.device)\n\t\tx = self.linear(x)\n\t\tx = F.normalize(x, p=2, dim=1)\n\n\t\treturn x\n\n\nclass ContrastLoss(nn.Module):\n\t'''\n\tcontrastive loss, corresponding to Eq.(18)\n\t'''\n\tdef __init__(self, n_data, eps=1e-7):\n\t\tsuper(ContrastLoss, self).__init__()\n\t\tself.n_data = n_data\n\t\tself.eps = eps\n\n\tdef forward(self, x):\n\t\tbs = x.size(0)\n\t\tN  = x.size(1) - 1\n\t\tM  = float(self.n_data)\n\n\t\t# loss for positive pair\n\t\tpos_pair = x.select(1, 0)\n\t\tlog_pos  = torch.div(pos_pair, pos_pair.add(N / M + self.eps)).log_()\n\n\t\t# loss for negative pair\n\t\tneg_pair = x.narrow(1, 1, N)\n\t\tlog_neg  = torch.div(neg_pair.clone().fill_(N / M), neg_pair.add(N / M + self.eps)).log_()\n\n\t\tloss = -(log_pos.sum() + log_neg.sum()) / bs\n\n\t\treturn loss\n\n\nclass ContrastMemory(nn.Module):\n    def __init__(self, feat_dim, n_data, nce_n, nce_t, nce_mom):\n        super(ContrastMemory, self).__init__()\n        self.N = nce_n\n        self.T = nce_t\n        self.momentum = nce_mom\n        self.Z_t = None\n        self.Z_s = None\n\n        stdv = 1. / math.sqrt(feat_dim / 3.)\n        self.register_buffer('memory_t', torch.rand(n_data, feat_dim).mul_(2 * stdv).add_(-stdv))\n        self.register_buffer('memory_s', torch.rand(n_data, feat_dim).mul_(2 * stdv).add_(-stdv))\n\n    def forward(self, feat_s, feat_t, idx, sample_idx):\n        bs = feat_s.size(0)\n        feat_dim = self.memory_s.size(1)\n        n_data = self.memory_s.size(0)\n        sample_idx = sample_idx.to(self.memory_s.device)\n\n        # using teacher as anchor\n        weight_s = torch.index_select(self.memory_s, 0, sample_idx.view(-1)).detach()\n        weight_s = weight_s.view(bs, self.N + 1, feat_dim)\n        out_t = torch.bmm(weight_s, feat_t.view(bs, feat_dim, 1)).squeeze().contiguous()\n        out_t = torch.exp(out_t / self.T)\n\n        # using student as anchor\n        weight_t = torch.index_select(self.memory_t, 0, sample_idx.view(-1)).detach()\n        weight_t = weight_t.view(bs, self.N + 1, feat_dim)\n        out_s = torch.bmm(weight_t, feat_s.view(bs, feat_dim, 1)).squeeze().contiguous()\n        out_s = torch.exp(out_s / self.T)\n\n        # set Z if haven't been set yet\n        if self.Z_t is None:\n            self.Z_t = (out_t.mean() * n_data).detach().item()\n        if self.Z_s is None:\n            self.Z_s = (out_s.mean() * n_data).detach().item()\n\n        out_t = torch.div(out_t, self.Z_t)\n        out_s = torch.div(out_s, self.Z_s)\n\n        # update memory\n        with torch.no_grad():\n            idx = idx.to(self.memory_t.device)\n            pos_mem_t = torch.index_select(self.memory_t, 0, idx.view(-1))\n            pos_mem_t.mul_(self.momentum)\n            pos_mem_t.add_(torch.mul(feat_t, 1 - self.momentum))\n            pos_mem_t = F.normalize(pos_mem_t, p=2, dim=1)\n            self.memory_t.index_copy_(0, idx, pos_mem_t)\n\n            pos_mem_s = torch.index_select(self.memory_s, 0, idx.view(-1))\n            pos_mem_s.mul_(self.momentum)\n            pos_mem_s.add_(torch.mul(feat_s, 1 - self.momentum))\n            pos_mem_s = F.normalize(pos_mem_s, p=2, dim=1)\n            self.memory_s.index_copy_(0, idx, pos_mem_s)\n\n        return out_s, out_t\n","metadata":{"id":"ECNCISMDYyOA","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:57:07.993611Z","iopub.execute_input":"2024-12-25T15:57:07.993897Z","iopub.status.idle":"2024-12-25T15:57:08.010009Z","shell.execute_reply.started":"2024-12-25T15:57:07.993875Z","shell.execute_reply":"2024-12-25T15:57:08.009199Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_crd_transform = transforms.Compose([\n    transforms.Resize((224, 224)),   # <-- add this\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntrain_crd_dataset = CIFAR10IdxSample\n\ntrain_crd_loader = torch.utils.data.DataLoader(\n\t\t\ttrain_crd_dataset(\n              root  = './datasets',\n\t\t\t\t\t\t  transform = train_crd_transform,\n\t\t\t\t\t\t  train     = True,\n\t\t\t\t\t\t  download  = True,\n\t\t\t\t\t\t  n         = 4096,\n\t\t\t\t\t\t  mode      = 'exact'\n      ),\n\t\t\tbatch_size=32, shuffle=True, num_workers=4, pin_memory=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ItP37hgbY492","outputId":"e03af9ba-ed7e-4106-9625-6285d1dde37b","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:57:13.365648Z","iopub.execute_input":"2024-12-25T15:57:13.365921Z","iopub.status.idle":"2024-12-25T15:57:28.477804Z","shell.execute_reply.started":"2024-12-25T15:57:13.365901Z","shell.execute_reply":"2024-12-25T15:57:28.477050Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:11<00:00, 14751851.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./datasets/cifar-10-python.tar.gz to ./datasets\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def train_student_crd(student, features_student, teacher, features_teacher, train_loader, kd, num_epochs=10, alpha=1e-3):\n    \"\"\"Train the model on the training data for CRD \"\"\"\n    print(\"Distilling the knowledge and training the student on this data for CRD\")\n    student.train()\n    teacher.eval()\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(student.parameters(), lr=alpha)\n    num_batches = len(train_loader)\n\n    for epoch in range(num_epochs):\n      running_loss = 0.0\n      correct = 0\n      total = 0\n      for batch_index, (img, target, idx, sample_idx ) in enumerate(train_loader, start=1):\n        # print(img.shape, target.shape, idx.shape, sample_idx.shape)\n        img = img.to(device)\n        target = target.to(device)\n        idx = idx.to(device)\n        sample_idx = sample_idx.to(device)\n\n        #get features and logits\n        # print(img.shape)\n        feat_s, logit_s = get_features_and_logits(student,features_student,img)\n        feat_t, _ = get_features_and_logits(teacher,features_teacher,img,student=False, crd=True)\n\n\n        feat_s = feat_s.to(device)\n        feat_t = feat_t.to(device)\n        logit_s = logit_s.to(device)\n\n\n        #get total loss\n        kd_lambda = 1   # 0.2   #for CRD\n        # print(feat_t.shape, feat_s.shape)\n        kd_loss  = kd(feat_s, feat_t, idx, sample_idx) * kd_lambda\n        cls_loss = criterion(logit_s, target)\n        total_loss = cls_loss + kd_loss\n        # print(\"total loss\",total_loss, \"crd + cls\", kd_loss , cls_loss)\n\n\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        running_loss += total_loss.item()\n\n        # # Calculate accuracy\n        # _, predicted = torch.max(logit_s, 1)\n        # correct += (predicted == labels).sum().item()\n        # total += labels.size(0)\n\n        if batch_index % 200 == 0 or batch_index in [1,750]:\n            print(f'Batch {batch_index }/{num_batches} - Loss: {total_loss.item():.4f} ')\n            # Accuracy: {(correct/total) * 100:.2f}%')\n\n      epoch_loss = running_loss / len(train_loader)\n      # epoch_accuracy = correct / total\n      print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n    # return epoch_loss, epoch_accuracy\n    return epoch_loss\n\n\n# criterion_CRD  = CRD(s_dim=get_channel_num(student_model,'vit'), t_dim=get_channel_num(teacher_model,'resnet'), n_data=len(train_loader.dataset))\n# train_student_crd(student_model, features_student, teacher_model, features_teacher, train_crd_loader, criterion_CRD, num_epochs=5)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QU54u6dsZ4Xq","outputId":"46f295c1-d759-4551-d967-2a64b942bcde","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:57:28.814924Z","iopub.execute_input":"2024-12-25T15:57:28.815212Z","iopub.status.idle":"2024-12-25T15:57:28.822561Z","shell.execute_reply.started":"2024-12-25T15:57:28.815186Z","shell.execute_reply":"2024-12-25T15:57:28.821652Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Texture Bias Dataset","metadata":{"id":"r8bpl3L4XooK"}},{"cell_type":"code","source":"# Define the dataset path\nbase_dir = '/kaggle/input/texture-bias-dataset/'  # Update this path based on your Kaggle dataset location\n\n# Function to get all partition paths\ndef get_partition_paths(base_dir):\n    # Navigate to the deeper subdirectory containing images and labels.txt\n    return [os.path.join(base_dir, f\"partition{i}\", f\"partition{i}\") for i in range(1, 11)]\n\n# Function to load images and labels from a single partition\ndef load_partition_data(partition_path):\n    images = []\n    labels = []\n    \n    # Path to the labels.txt file\n    label_file = os.path.join(partition_path, 'labels.txt')\n    \n    # Read the labels.txt file\n    with open(label_file, 'r') as f:\n        for line in f:\n            img_name, label = line.strip().split()  # Split filename and label\n            img_path = os.path.join(partition_path, img_name)  # Full path to image\n            label = int(label)  # Convert label to integer\n            \n            # Load the image\n            img = Image.open(img_path).convert('RGB')  # Ensure RGB format\n            \n            images.append((img_path, img))  # Store tuple of (image path, image object)\n            labels.append(label)\n    \n    return images, labels\n\n# Load data from all partitions\nall_images = []  # To store tuples of (image path, image data)\nall_labels = []  # To store corresponding labels\n\nfor partition_path in get_partition_paths(base_dir):\n    images, labels = load_partition_data(partition_path)\n    all_images.extend(images)\n    all_labels.extend(labels)\n\n# Print some debug info\nprint(f\"Loaded {len(all_images)} images and {len(all_labels)} labels.\")\nprint(f\"Example Image Path: {all_images[0][0]}, Label: {all_labels[0]}\")\n\n# # Preprocessing for PyTorch\n# transform = transforms.Compose([\n#     transforms.Resize((32, 32)),  # CIFAR-10 images are 32x32\n#     transforms.ToTensor(),       # Convert image to tensor\n# ])\n\n# Apply transformations to images\nprocessed_images = [transform(img[1]) for img in all_images]  # img[1] contains the actual image data\nall_labels = torch.tensor(all_labels)  # Convert labels to tensor\n\n# Create a PyTorch DataLoader\ntexture_bias_dataset = TensorDataset(torch.stack(processed_images), all_labels)\ntexture_bias_loader = DataLoader(texture_bias_dataset, batch_size=32, shuffle=False)","metadata":{"id":"BJAa-tjRqo6M","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:24:32.859012Z","iopub.execute_input":"2024-12-25T17:24:32.859326Z","iopub.status.idle":"2024-12-25T17:26:48.656248Z","shell.execute_reply.started":"2024-12-25T17:24:32.859295Z","shell.execute_reply":"2024-12-25T17:26:48.655204Z"}},"outputs":[{"name":"stdout","text":"Loaded 10000 images and 10000 labels.\nExample Image Path: /kaggle/input/texture-bias-dataset/partition1/partition1/img_0000.png, Label: 3\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Finetuning Teacher(Resnet50) on Cifar10 dataset\n","metadata":{}},{"cell_type":"code","source":"# 1. Get the teacher model (ResNet) and its features\nteacher_model, features_teacher = get_model('teacher')\n\n# Move the teacher model to the correct device\nteacher_model = teacher_model.to(device)\n\n\n# 2. Finetune the teacher on CIFAR-10\nteacher_loss, teacher_accuracy = finetune_model(\n    teacher_model, \n    train_loader,     # CIFAR-10 train_loader\n    num_epochs=10, \n    alpha=1e-3\n)\n\nprint(f\"Teacher finetuned on CIFAR-10 with final loss={teacher_loss:.4f}, accuracy={teacher_accuracy*100:.2f}%\")\n\ntorch.save(teacher_model.state_dict(), 'teacher_model_weights_after_finetuning.pth')\n\nprint(\"Teacher model weights saved successfully!\")\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T15:41:35.364329Z","iopub.execute_input":"2024-12-22T15:41:35.364653Z","iopub.status.idle":"2024-12-22T16:17:12.431000Z","shell.execute_reply.started":"2024-12-22T15:41:35.364632Z","shell.execute_reply":"2024-12-22T16:17:12.430014Z"}},"outputs":[{"name":"stdout","text":"Finetuning the model on this data\nBatch 1/1563 - Loss: 2.3532 Accuracy: 12.50%\nBatch 301/1563 - Loss: 1.1407 Accuracy: 65.32%\nBatch 601/1563 - Loss: 0.7251 Accuracy: 70.28%\nBatch 751/1563 - Loss: 0.6909 Accuracy: 71.39%\nBatch 901/1563 - Loss: 1.1071 Accuracy: 71.98%\nBatch 1201/1563 - Loss: 0.8286 Accuracy: 73.11%\nBatch 1501/1563 - Loss: 0.7037 Accuracy: 73.95%\n------------------------->Epoch [1/10], Loss: 0.7639 Accuracy : 74.09%\nBatch 1/1563 - Loss: 0.6780 Accuracy: 75.00%\nBatch 301/1563 - Loss: 0.4301 Accuracy: 78.04%\nBatch 601/1563 - Loss: 1.0053 Accuracy: 78.02%\nBatch 751/1563 - Loss: 0.5594 Accuracy: 78.00%\nBatch 901/1563 - Loss: 0.8027 Accuracy: 78.04%\nBatch 1201/1563 - Loss: 0.3043 Accuracy: 78.23%\nBatch 1501/1563 - Loss: 0.6332 Accuracy: 78.32%\n------------------------->Epoch [2/10], Loss: 0.6251 Accuracy : 78.38%\nBatch 1/1563 - Loss: 0.6996 Accuracy: 68.75%\nBatch 301/1563 - Loss: 0.4643 Accuracy: 79.51%\nBatch 601/1563 - Loss: 0.7940 Accuracy: 79.00%\nBatch 751/1563 - Loss: 0.4711 Accuracy: 79.15%\nBatch 901/1563 - Loss: 0.9213 Accuracy: 79.19%\nBatch 1201/1563 - Loss: 0.2838 Accuracy: 79.22%\nBatch 1501/1563 - Loss: 0.2955 Accuracy: 79.34%\n------------------------->Epoch [3/10], Loss: 0.6066 Accuracy : 79.34%\nBatch 1/1563 - Loss: 0.9583 Accuracy: 65.62%\nBatch 301/1563 - Loss: 0.5024 Accuracy: 79.27%\nBatch 601/1563 - Loss: 0.6348 Accuracy: 79.84%\nBatch 751/1563 - Loss: 0.8969 Accuracy: 79.89%\nBatch 901/1563 - Loss: 0.6861 Accuracy: 79.85%\nBatch 1201/1563 - Loss: 0.3531 Accuracy: 79.99%\nBatch 1501/1563 - Loss: 0.2907 Accuracy: 79.89%\n------------------------->Epoch [4/10], Loss: 0.5862 Accuracy : 79.88%\nBatch 1/1563 - Loss: 0.3087 Accuracy: 87.50%\nBatch 301/1563 - Loss: 0.5863 Accuracy: 80.63%\nBatch 601/1563 - Loss: 0.5216 Accuracy: 80.29%\nBatch 751/1563 - Loss: 0.4568 Accuracy: 80.21%\nBatch 901/1563 - Loss: 0.4038 Accuracy: 80.16%\nBatch 1201/1563 - Loss: 0.6505 Accuracy: 80.27%\nBatch 1501/1563 - Loss: 0.8012 Accuracy: 80.25%\n------------------------->Epoch [5/10], Loss: 0.5766 Accuracy : 80.23%\nBatch 1/1563 - Loss: 0.6940 Accuracy: 71.88%\nBatch 301/1563 - Loss: 0.4417 Accuracy: 81.30%\nBatch 601/1563 - Loss: 0.2106 Accuracy: 80.83%\nBatch 751/1563 - Loss: 0.6128 Accuracy: 80.69%\nBatch 901/1563 - Loss: 0.2266 Accuracy: 80.74%\nBatch 1201/1563 - Loss: 0.4927 Accuracy: 80.80%\nBatch 1501/1563 - Loss: 0.5919 Accuracy: 80.77%\n------------------------->Epoch [6/10], Loss: 0.5593 Accuracy : 80.78%\nBatch 1/1563 - Loss: 0.7632 Accuracy: 75.00%\nBatch 301/1563 - Loss: 0.4731 Accuracy: 80.33%\nBatch 601/1563 - Loss: 0.3516 Accuracy: 80.45%\nBatch 751/1563 - Loss: 0.4359 Accuracy: 80.58%\nBatch 901/1563 - Loss: 0.4644 Accuracy: 80.66%\nBatch 1201/1563 - Loss: 0.4255 Accuracy: 80.80%\nBatch 1501/1563 - Loss: 0.8695 Accuracy: 80.66%\n------------------------->Epoch [7/10], Loss: 0.5552 Accuracy : 80.65%\nBatch 1/1563 - Loss: 0.4780 Accuracy: 84.38%\nBatch 301/1563 - Loss: 0.4118 Accuracy: 81.47%\nBatch 601/1563 - Loss: 1.0135 Accuracy: 81.34%\nBatch 751/1563 - Loss: 0.6028 Accuracy: 81.24%\nBatch 901/1563 - Loss: 0.6720 Accuracy: 81.17%\nBatch 1201/1563 - Loss: 0.8769 Accuracy: 81.17%\nBatch 1501/1563 - Loss: 0.7218 Accuracy: 81.21%\n------------------------->Epoch [8/10], Loss: 0.5420 Accuracy : 81.27%\nBatch 1/1563 - Loss: 0.3400 Accuracy: 84.38%\nBatch 301/1563 - Loss: 0.8713 Accuracy: 81.89%\nBatch 601/1563 - Loss: 0.7106 Accuracy: 81.61%\nBatch 751/1563 - Loss: 0.3954 Accuracy: 81.43%\nBatch 901/1563 - Loss: 0.5241 Accuracy: 81.20%\nBatch 1201/1563 - Loss: 0.4074 Accuracy: 81.41%\nBatch 1501/1563 - Loss: 0.5678 Accuracy: 81.39%\n------------------------->Epoch [9/10], Loss: 0.5362 Accuracy : 81.38%\nBatch 1/1563 - Loss: 0.5332 Accuracy: 81.25%\nBatch 301/1563 - Loss: 0.4638 Accuracy: 82.42%\nBatch 601/1563 - Loss: 0.6759 Accuracy: 82.22%\nBatch 751/1563 - Loss: 0.3115 Accuracy: 82.08%\nBatch 901/1563 - Loss: 0.7639 Accuracy: 81.96%\nBatch 1201/1563 - Loss: 0.5051 Accuracy: 81.87%\nBatch 1501/1563 - Loss: 0.5101 Accuracy: 81.84%\n------------------------->Epoch [10/10], Loss: 0.5251 Accuracy : 81.82%\nTeacher finetuned on CIFAR-10 with final loss=0.5251, accuracy=81.82%\nTeacher model weights saved successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Evaluating Teacher on Texture Bias Dataset","metadata":{}},{"cell_type":"code","source":"# ---------------code for loading wieghts of the finetuned teacher model--------------------------\n# # Load the state_dict into the model\n# 1. Get the teacher model (ResNet) and its features\nteacher_model, features_teacher = get_model('teacher')\n\n# Move the teacher model to the correct device\nteacher_model = teacher_model.to(device)\n\nteacher_model.load_state_dict(torch.load('/kaggle/input/after-finetuning-teacher/teacher_model_weights_after_finetuning.pth'))\n\n# Set the model to evaluation mode if you're using it for inference\nteacher_model.eval()\n\nprint(\"Teacher model weights loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:26:48.657770Z","iopub.execute_input":"2024-12-25T17:26:48.658157Z","iopub.status.idle":"2024-12-25T17:26:51.104676Z","shell.execute_reply.started":"2024-12-25T17:26:48.658123Z","shell.execute_reply":"2024-12-25T17:26:51.103969Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 204MB/s]\n<ipython-input-6-306531f89266>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  teacher_model.load_state_dict(torch.load('/kaggle/input/after-finetuning-teacher/teacher_model_weights_after_finetuning.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Teacher model weights loaded successfully!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ---------------code for loading wieghts of the finetuned teacher model--------------------------\n# # Load the state_dict into the model\n# teacher_model.load_state_dict(torch.load('teacher_model_weights_after_finetuning.pth'))\n\n# # Set the model to evaluation mode if you're using it for inference\n# teacher_model.eval()\n\n# print(\"Teacher model weights loaded successfully!\")\n\n\ntexture_teacher_acc = evaluate_model(teacher_model, texture_bias_loader, device)\nprint(f\"Teacher accuracy on Texture-Bias dataset = {texture_teacher_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T16:18:13.981617Z","iopub.execute_input":"2024-12-22T16:18:13.981990Z","iopub.status.idle":"2024-12-22T16:18:46.575951Z","shell.execute_reply.started":"2024-12-22T16:18:13.981932Z","shell.execute_reply":"2024-12-22T16:18:46.575126Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 12.50%\nBatch 11/313 - Accuracy: 17.90%\nBatch 21/313 - Accuracy: 15.92%\nBatch 31/313 - Accuracy: 16.23%\nBatch 41/313 - Accuracy: 15.47%\nBatch 51/313 - Accuracy: 15.07%\nBatch 61/313 - Accuracy: 14.75%\nBatch 71/313 - Accuracy: 14.92%\nBatch 81/313 - Accuracy: 15.08%\nBatch 91/313 - Accuracy: 15.08%\nBatch 101/313 - Accuracy: 14.82%\nBatch 111/313 - Accuracy: 14.86%\nBatch 121/313 - Accuracy: 14.90%\nBatch 131/313 - Accuracy: 15.12%\nBatch 141/313 - Accuracy: 15.18%\nBatch 151/313 - Accuracy: 15.00%\nBatch 161/313 - Accuracy: 15.06%\nBatch 171/313 - Accuracy: 15.08%\nBatch 181/313 - Accuracy: 15.18%\nBatch 191/313 - Accuracy: 15.09%\nBatch 201/313 - Accuracy: 15.02%\nBatch 211/313 - Accuracy: 15.20%\nBatch 221/313 - Accuracy: 15.19%\nBatch 231/313 - Accuracy: 15.12%\nBatch 241/313 - Accuracy: 15.22%\nBatch 251/313 - Accuracy: 15.35%\nBatch 261/313 - Accuracy: 15.16%\nBatch 271/313 - Accuracy: 15.16%\nBatch 281/313 - Accuracy: 15.11%\nBatch 291/313 - Accuracy: 15.05%\nBatch 301/313 - Accuracy: 15.01%\nBatch 311/313 - Accuracy: 15.09%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 15.14%\nTeacher accuracy on Texture-Bias dataset = 15.14%\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Knowledge Distillation (Logit matching)","metadata":{}},{"cell_type":"code","source":"# Suppose you define kd = LogitMatching(T=3.0)\nstudent_lm, features_lm = get_model('student')  # ViT\n\nstudent_lm = student_lm.to(device)\n\nlogit_matching_loss = LogitMatching(T=3.0)       # or whatever T you want\n\n# distill knowledge\ntrain_student(\n    student=student_lm,\n    teacher=teacher_model,\n    train_loader=train_loader,  # still CIFAR-10\n    kd=logit_matching_loss,\n    num_epochs=10,\n    alpha=1e-3,\n    features_student=features_lm,\n    features_teacher=features_teacher\n)\n\n# Save the teacher model weights\ntorch.save(teacher_model.state_dict(), 'teacher_model_weights.pth')\n\n# Save the student model weights\ntorch.save(student_lm.state_dict(), 'student_model_weights.pth')\n\nprint(\"Teacher and student model weights saved successfully!\")\n\n# Optionally save the features (if they need to be reused)\ntorch.save(features_teacher, 'features_teacher.pth')\ntorch.save(features_lm, 'features_student.pth')\n\nprint(\"Features saved successfully!\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T16:22:51.039746Z","iopub.execute_input":"2024-12-22T16:22:51.040061Z","iopub.status.idle":"2024-12-22T18:38:49.515336Z","shell.execute_reply.started":"2024-12-22T16:22:51.040039Z","shell.execute_reply":"2024-12-22T18:38:49.514523Z"}},"outputs":[{"name":"stdout","text":"Distilling the knowledge and training the student on this data LogitMatching\nBatch 1/1563 - Loss: 12.1133 Accuracy: 6.25%\nBatch 201/1563 - Loss: 1.3886 Accuracy: 88.60%\nBatch 401/1563 - Loss: 1.9707 Accuracy: 90.65%\nBatch 601/1563 - Loss: 1.5134 Accuracy: 91.44%\nBatch 751/1563 - Loss: 1.6779 Accuracy: 91.75%\nBatch 801/1563 - Loss: 1.4647 Accuracy: 91.84%\nBatch 1001/1563 - Loss: 1.5533 Accuracy: 92.12%\nBatch 1201/1563 - Loss: 1.3349 Accuracy: 92.26%\nBatch 1401/1563 - Loss: 1.7039 Accuracy: 92.41%\n------------------------->Epoch [1/10], Loss: 1.7205 Accuracy : 92.50%\nBatch 1/1563 - Loss: 1.6260 Accuracy: 93.75%\nBatch 201/1563 - Loss: 1.3967 Accuracy: 93.72%\nBatch 401/1563 - Loss: 1.4239 Accuracy: 93.76%\nBatch 601/1563 - Loss: 1.2717 Accuracy: 93.50%\nBatch 751/1563 - Loss: 1.2137 Accuracy: 93.49%\nBatch 801/1563 - Loss: 1.5362 Accuracy: 93.47%\nBatch 1001/1563 - Loss: 1.5717 Accuracy: 93.46%\nBatch 1201/1563 - Loss: 1.1261 Accuracy: 93.40%\nBatch 1401/1563 - Loss: 2.0002 Accuracy: 93.41%\n------------------------->Epoch [2/10], Loss: 1.5330 Accuracy : 93.40%\nBatch 1/1563 - Loss: 1.7185 Accuracy: 93.75%\nBatch 201/1563 - Loss: 1.6946 Accuracy: 93.73%\nBatch 401/1563 - Loss: 1.2766 Accuracy: 93.59%\nBatch 601/1563 - Loss: 1.4243 Accuracy: 93.48%\nBatch 751/1563 - Loss: 1.2815 Accuracy: 93.38%\nBatch 801/1563 - Loss: 1.5506 Accuracy: 93.39%\nBatch 1001/1563 - Loss: 1.3050 Accuracy: 93.39%\nBatch 1201/1563 - Loss: 1.5503 Accuracy: 93.44%\nBatch 1401/1563 - Loss: 1.3300 Accuracy: 93.50%\n------------------------->Epoch [3/10], Loss: 1.5111 Accuracy : 93.51%\nBatch 1/1563 - Loss: 1.5045 Accuracy: 96.88%\nBatch 201/1563 - Loss: 1.3335 Accuracy: 93.77%\nBatch 401/1563 - Loss: 1.8714 Accuracy: 93.88%\nBatch 601/1563 - Loss: 1.5900 Accuracy: 93.78%\nBatch 751/1563 - Loss: 1.4687 Accuracy: 93.66%\nBatch 801/1563 - Loss: 1.5296 Accuracy: 93.67%\nBatch 1001/1563 - Loss: 1.3606 Accuracy: 93.63%\nBatch 1201/1563 - Loss: 1.6884 Accuracy: 93.65%\nBatch 1401/1563 - Loss: 1.3757 Accuracy: 93.49%\n------------------------->Epoch [4/10], Loss: 1.5000 Accuracy : 93.53%\nBatch 1/1563 - Loss: 1.6142 Accuracy: 84.38%\nBatch 201/1563 - Loss: 1.2969 Accuracy: 93.24%\nBatch 401/1563 - Loss: 1.4453 Accuracy: 93.58%\nBatch 601/1563 - Loss: 1.1814 Accuracy: 93.42%\nBatch 751/1563 - Loss: 1.5560 Accuracy: 93.43%\nBatch 801/1563 - Loss: 1.4690 Accuracy: 93.46%\nBatch 1001/1563 - Loss: 1.6978 Accuracy: 93.53%\nBatch 1201/1563 - Loss: 1.6175 Accuracy: 93.47%\nBatch 1401/1563 - Loss: 1.0500 Accuracy: 93.45%\n------------------------->Epoch [5/10], Loss: 1.4970 Accuracy : 93.42%\nBatch 1/1563 - Loss: 1.8967 Accuracy: 90.62%\nBatch 201/1563 - Loss: 1.2559 Accuracy: 93.72%\nBatch 401/1563 - Loss: 1.3334 Accuracy: 93.60%\nBatch 601/1563 - Loss: 1.6917 Accuracy: 93.51%\nBatch 751/1563 - Loss: 1.2266 Accuracy: 93.63%\nBatch 801/1563 - Loss: 1.5540 Accuracy: 93.59%\nBatch 1001/1563 - Loss: 1.3561 Accuracy: 93.49%\nBatch 1201/1563 - Loss: 1.4790 Accuracy: 93.49%\nBatch 1401/1563 - Loss: 1.1838 Accuracy: 93.44%\n------------------------->Epoch [6/10], Loss: 1.4930 Accuracy : 93.45%\nBatch 1/1563 - Loss: 1.6871 Accuracy: 93.75%\nBatch 201/1563 - Loss: 1.3060 Accuracy: 93.91%\nBatch 401/1563 - Loss: 1.4305 Accuracy: 93.59%\nBatch 601/1563 - Loss: 1.6477 Accuracy: 93.65%\nBatch 751/1563 - Loss: 1.2177 Accuracy: 93.70%\nBatch 801/1563 - Loss: 1.7620 Accuracy: 93.67%\nBatch 1001/1563 - Loss: 2.1747 Accuracy: 93.70%\nBatch 1201/1563 - Loss: 1.5724 Accuracy: 93.65%\nBatch 1401/1563 - Loss: 1.4521 Accuracy: 93.58%\n------------------------->Epoch [7/10], Loss: 1.4883 Accuracy : 93.57%\nBatch 1/1563 - Loss: 1.6090 Accuracy: 87.50%\nBatch 201/1563 - Loss: 1.2639 Accuracy: 93.63%\nBatch 401/1563 - Loss: 1.8646 Accuracy: 93.73%\nBatch 601/1563 - Loss: 1.7248 Accuracy: 93.59%\nBatch 751/1563 - Loss: 2.0343 Accuracy: 93.62%\nBatch 801/1563 - Loss: 1.5155 Accuracy: 93.64%\nBatch 1001/1563 - Loss: 1.6535 Accuracy: 93.53%\nBatch 1201/1563 - Loss: 1.1976 Accuracy: 93.51%\nBatch 1401/1563 - Loss: 1.7675 Accuracy: 93.53%\n------------------------->Epoch [8/10], Loss: 1.4874 Accuracy : 93.49%\nBatch 1/1563 - Loss: 1.6407 Accuracy: 90.62%\nBatch 201/1563 - Loss: 1.4024 Accuracy: 94.00%\nBatch 401/1563 - Loss: 1.7439 Accuracy: 93.64%\nBatch 601/1563 - Loss: 1.8614 Accuracy: 93.63%\nBatch 751/1563 - Loss: 1.5507 Accuracy: 93.65%\nBatch 801/1563 - Loss: 1.3648 Accuracy: 93.61%\nBatch 1001/1563 - Loss: 1.2704 Accuracy: 93.51%\nBatch 1201/1563 - Loss: 1.3507 Accuracy: 93.51%\nBatch 1401/1563 - Loss: 1.5621 Accuracy: 93.48%\n------------------------->Epoch [9/10], Loss: 1.4872 Accuracy : 93.50%\nBatch 1/1563 - Loss: 1.3885 Accuracy: 96.88%\nBatch 201/1563 - Loss: 1.2441 Accuracy: 93.78%\nBatch 401/1563 - Loss: 1.6539 Accuracy: 93.82%\nBatch 601/1563 - Loss: 1.0398 Accuracy: 93.66%\nBatch 751/1563 - Loss: 1.0484 Accuracy: 93.75%\nBatch 801/1563 - Loss: 1.6188 Accuracy: 93.77%\nBatch 1001/1563 - Loss: 1.2887 Accuracy: 93.69%\nBatch 1201/1563 - Loss: 1.6951 Accuracy: 93.64%\nBatch 1401/1563 - Loss: 1.4460 Accuracy: 93.61%\n------------------------->Epoch [10/10], Loss: 1.4883 Accuracy : 93.58%\nTeacher and student model weights saved successfully!\nFeatures saved successfully!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Evaluating Logit matching student on Texture Bias dataset","metadata":{}},{"cell_type":"code","source":"# Reload the teacher model\nteacher_model, features_teacher = get_model('teacher')  # Initialize the teacher architecture\nteacher_model.load_state_dict(torch.load('/kaggle/input/logit-matching/teacher_model_weights.pth'))\nteacher_model.eval()  # Set to evaluation mode\n\n# # Reload the student model\nstudent_lm, features_lm = get_model('student')  # Initialize the student architecture\nstudent_lm.load_state_dict(torch.load('/kaggle/input/logit-matching/student_model_weights.pth'))\nstudent_lm.eval()  # Set to evaluation mode\n\n# # Optionally load the features\nfeatures_teacher = torch.load('/kaggle/input/logit-matching/features_teacher.pth')\nfeatures_lm = torch.load('/kaggle/input/logit-matching/features_student.pth')\n\n# print(\"Teacher and student models with features loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:04:45.097720Z","iopub.execute_input":"2024-12-25T16:04:45.098046Z","iopub.status.idle":"2024-12-25T16:04:57.205015Z","shell.execute_reply.started":"2024-12-25T16:04:45.098023Z","shell.execute_reply":"2024-12-25T16:04:57.204325Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-16-0bcb5249b112>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  teacher_model.load_state_dict(torch.load('/kaggle/input/logit-matching/teacher_model_weights.pth'))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d303da431334f4e886d052bfc04502f"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-16-0bcb5249b112>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student_lm.load_state_dict(torch.load('/kaggle/input/logit-matching/student_model_weights.pth'))\n<ipython-input-16-0bcb5249b112>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  features_teacher = torch.load('/kaggle/input/logit-matching/features_teacher.pth')\n<ipython-input-16-0bcb5249b112>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  features_lm = torch.load('/kaggle/input/logit-matching/features_student.pth')\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"test_student_lm_acc = evaluate_model(student_lm, test_loader, device)\nprint(f\"Student (LogitMatching) accuracy on Cifar 10 test set = {test_student_lm_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:07:31.282721Z","iopub.execute_input":"2024-12-25T16:07:31.283048Z","iopub.status.idle":"2024-12-25T16:09:41.215890Z","shell.execute_reply.started":"2024-12-25T16:07:31.283026Z","shell.execute_reply":"2024-12-25T16:09:41.215017Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 100.00%\nBatch 11/313 - Accuracy: 93.47%\nBatch 21/313 - Accuracy: 93.45%\nBatch 31/313 - Accuracy: 93.55%\nBatch 41/313 - Accuracy: 93.60%\nBatch 51/313 - Accuracy: 93.26%\nBatch 61/313 - Accuracy: 93.29%\nBatch 71/313 - Accuracy: 93.22%\nBatch 81/313 - Accuracy: 93.13%\nBatch 91/313 - Accuracy: 93.17%\nBatch 101/313 - Accuracy: 93.01%\nBatch 111/313 - Accuracy: 92.91%\nBatch 121/313 - Accuracy: 92.95%\nBatch 131/313 - Accuracy: 92.82%\nBatch 141/313 - Accuracy: 93.00%\nBatch 151/313 - Accuracy: 92.82%\nBatch 161/313 - Accuracy: 92.76%\nBatch 171/313 - Accuracy: 92.82%\nBatch 181/313 - Accuracy: 92.83%\nBatch 191/313 - Accuracy: 92.82%\nBatch 201/313 - Accuracy: 92.79%\nBatch 211/313 - Accuracy: 92.65%\nBatch 221/313 - Accuracy: 92.56%\nBatch 231/313 - Accuracy: 92.71%\nBatch 241/313 - Accuracy: 92.58%\nBatch 251/313 - Accuracy: 92.62%\nBatch 261/313 - Accuracy: 92.60%\nBatch 271/313 - Accuracy: 92.60%\nBatch 281/313 - Accuracy: 92.64%\nBatch 291/313 - Accuracy: 92.62%\nBatch 301/313 - Accuracy: 92.67%\nBatch 311/313 - Accuracy: 92.67%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 92.68%\nStudent (LogitMatching) accuracy on Cifar 10 test set = 92.68%\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"\n# Evaluate on texture-bias\ntexture_student_lm_acc = evaluate_model(student_lm, texture_bias_loader, device)\nprint(f\"Student (LogitMatching) accuracy on Texture-Bias = {texture_student_lm_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:40:20.946561Z","iopub.execute_input":"2024-12-22T18:40:20.946888Z","iopub.status.idle":"2024-12-22T18:42:24.686212Z","shell.execute_reply.started":"2024-12-22T18:40:20.946864Z","shell.execute_reply":"2024-12-22T18:42:24.685281Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 37.50%\nBatch 11/313 - Accuracy: 33.24%\nBatch 21/313 - Accuracy: 32.89%\nBatch 31/313 - Accuracy: 33.17%\nBatch 41/313 - Accuracy: 33.84%\nBatch 51/313 - Accuracy: 33.82%\nBatch 61/313 - Accuracy: 33.86%\nBatch 71/313 - Accuracy: 33.54%\nBatch 81/313 - Accuracy: 33.49%\nBatch 91/313 - Accuracy: 33.76%\nBatch 101/313 - Accuracy: 33.69%\nBatch 111/313 - Accuracy: 33.87%\nBatch 121/313 - Accuracy: 34.01%\nBatch 131/313 - Accuracy: 34.16%\nBatch 141/313 - Accuracy: 34.40%\nBatch 151/313 - Accuracy: 34.60%\nBatch 161/313 - Accuracy: 34.78%\nBatch 171/313 - Accuracy: 34.83%\nBatch 181/313 - Accuracy: 34.88%\nBatch 191/313 - Accuracy: 34.80%\nBatch 201/313 - Accuracy: 34.70%\nBatch 211/313 - Accuracy: 34.58%\nBatch 221/313 - Accuracy: 34.39%\nBatch 231/313 - Accuracy: 34.40%\nBatch 241/313 - Accuracy: 34.30%\nBatch 251/313 - Accuracy: 34.31%\nBatch 261/313 - Accuracy: 34.23%\nBatch 271/313 - Accuracy: 34.12%\nBatch 281/313 - Accuracy: 34.26%\nBatch 291/313 - Accuracy: 34.05%\nBatch 301/313 - Accuracy: 34.15%\nBatch 311/313 - Accuracy: 34.04%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 34.05%\nStudent (LogitMatching) accuracy on Texture-Bias = 34.05%\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Shape Bias Dataset","metadata":{}},{"cell_type":"code","source":"# Path to the dataset and labels file\nfrom torch.utils.data import Dataset, DataLoader\n\ndata_dir = '/kaggle/input/shape-bias-dataset/shapes/'\nlabels_file = os.path.join(data_dir, 'labels.txt')\n\n# Custom dataset for Shape-Bias Dataset\nclass ShapeBiasDataset(Dataset):\n    def __init__(self, data_dir, labels_file, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        \n        # Read labels file\n        self.data = []\n        with open(labels_file, 'r') as f:\n            for line in f:\n                img_name, label = line.strip().split()\n                # Replace .pt with .png dynamically and apply a shift of +1\n                img_name = img_name.replace('.pt', '.png')\n                img_number = int(img_name.split('_')[1].split('.')[0]) + 1  # Shift the index by +1\n                img_name = f\"img_{img_number}.png\"\n                self.data.append((img_name, int(label)))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name, label = self.data[idx]\n        img_path = os.path.join(self.data_dir, img_name)\n        \n        # Load image\n        image = Image.open(img_path).convert('RGB')  # Ensure RGB format\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# # Define the custom transformations\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),  # Resize images for compatibility with larger models (e.g., ResNet)\n#     transforms.ToTensor(),         # Convert image to tensor\n#     transforms.Normalize(          # Normalize using CIFAR-10 statistics\n#         mean=(0.4914, 0.4822, 0.4465), \n#         std=(0.247, 0.2435, 0.2616)\n#     ),\n# ])\n\n# Create the dataset and DataLoader with the custom transformations\nshape_bias_dataset = ShapeBiasDataset(data_dir=data_dir, labels_file=labels_file, transform=transform)\nshape_bias_loader = DataLoader(shape_bias_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:27:44.101390Z","iopub.execute_input":"2024-12-25T17:27:44.101739Z","iopub.status.idle":"2024-12-25T17:27:44.143481Z","shell.execute_reply.started":"2024-12-25T17:27:44.101712Z","shell.execute_reply":"2024-12-25T17:27:44.142671Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Evaluating Logit matching student on Shape Bias dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on shape-bias\nshape_student_lm_acc = evaluate_model(student_lm, shape_bias_loader, device)\nprint(f\"Student (LogitMatching) accuracy on Shape-Bias = {shape_student_lm_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:55:31.031988Z","iopub.execute_input":"2024-12-22T18:55:31.032316Z","iopub.status.idle":"2024-12-22T18:58:28.776930Z","shell.execute_reply.started":"2024-12-22T18:55:31.032289Z","shell.execute_reply":"2024-12-22T18:58:28.776144Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 9.38%\nBatch 11/313 - Accuracy: 16.76%\nBatch 21/313 - Accuracy: 16.82%\nBatch 31/313 - Accuracy: 17.64%\nBatch 41/313 - Accuracy: 16.54%\nBatch 51/313 - Accuracy: 17.10%\nBatch 61/313 - Accuracy: 17.06%\nBatch 71/313 - Accuracy: 17.17%\nBatch 81/313 - Accuracy: 16.98%\nBatch 91/313 - Accuracy: 16.79%\nBatch 101/313 - Accuracy: 16.80%\nBatch 111/313 - Accuracy: 16.67%\nBatch 121/313 - Accuracy: 16.30%\nBatch 131/313 - Accuracy: 16.17%\nBatch 141/313 - Accuracy: 16.25%\nBatch 151/313 - Accuracy: 16.27%\nBatch 161/313 - Accuracy: 16.44%\nBatch 171/313 - Accuracy: 16.37%\nBatch 181/313 - Accuracy: 16.42%\nBatch 191/313 - Accuracy: 16.41%\nBatch 201/313 - Accuracy: 16.34%\nBatch 211/313 - Accuracy: 16.41%\nBatch 221/313 - Accuracy: 16.37%\nBatch 231/313 - Accuracy: 16.44%\nBatch 241/313 - Accuracy: 16.34%\nBatch 251/313 - Accuracy: 16.28%\nBatch 261/313 - Accuracy: 16.22%\nBatch 271/313 - Accuracy: 16.24%\nBatch 281/313 - Accuracy: 16.19%\nBatch 291/313 - Accuracy: 16.19%\nBatch 301/313 - Accuracy: 16.21%\nBatch 311/313 - Accuracy: 16.27%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 16.30%\nStudent (LogitMatching) accuracy on Texture-Bias = 16.30%\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Super Pixelated Dataset","metadata":{}},{"cell_type":"code","source":"# Path to the dataset and labels file\ndata_dir = '/kaggle/input/superpixelated-dataset/slic/'\nlabels_file = os.path.join(data_dir, 'labels.txt')  # Ensure the labels.txt file exists\n\n# Custom dataset for the Superpixelated Dataset\nclass SuperpixelDataset(Dataset):\n    def __init__(self, data_dir, labels_file, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        \n        # Read labels file\n        self.data = []\n        with open(labels_file, 'r') as f:\n            for line in f:\n                img_name, label = line.strip().split()\n                # Replace .pt with .png dynamically and remove leading zeros from the image name\n                img_name = img_name.replace('.pt', '.png')\n                img_number = int(img_name.split('_')[1].split('.')[0])  # Extract numeric part\n                img_name = f\"img_{img_number}.png\"  # Reformat without leading zeros\n                self.data.append((img_name, int(label)))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name, label = self.data[idx]\n        img_path = os.path.join(self.data_dir, img_name)\n        \n        # Load image\n        image = Image.open(img_path).convert('RGB')  # Ensure RGB format\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# # Define transformations (use the same transformations for consistency)\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),  # Resize images for compatibility with the model\n#     transforms.ToTensor(),         # Convert image to tensor\n#     transforms.Normalize(          # Normalize using CIFAR-10 statistics\n#         mean=(0.4914, 0.4822, 0.4465), \n#         std=(0.247, 0.2435, 0.2616)\n#     ),\n# ])\n\n# Create the dataset and DataLoader\nsuperpixel_dataset = SuperpixelDataset(data_dir=data_dir, labels_file=labels_file, transform=transform)\nsuperpixel_loader = DataLoader(superpixel_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:28:01.850149Z","iopub.execute_input":"2024-12-25T17:28:01.850503Z","iopub.status.idle":"2024-12-25T17:28:01.891096Z","shell.execute_reply.started":"2024-12-25T17:28:01.850471Z","shell.execute_reply":"2024-12-25T17:28:01.890432Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Evaluating Logit matching student on SuperPixelated dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on super pixelated dataset\nsuper_pixelated_student_lm_acc = evaluate_model(student_lm, superpixel_loader, device)\nprint(f\"Student (LogitMatching) accuracy on Super Pixelated = {super_pixelated_student_lm_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:02:34.540709Z","iopub.execute_input":"2024-12-22T19:02:34.541009Z","iopub.status.idle":"2024-12-22T19:06:04.555318Z","shell.execute_reply.started":"2024-12-22T19:02:34.540987Z","shell.execute_reply":"2024-12-22T19:06:04.554547Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 56.25%\nBatch 11/313 - Accuracy: 39.77%\nBatch 21/313 - Accuracy: 38.84%\nBatch 31/313 - Accuracy: 37.40%\nBatch 41/313 - Accuracy: 38.34%\nBatch 51/313 - Accuracy: 37.87%\nBatch 61/313 - Accuracy: 38.11%\nBatch 71/313 - Accuracy: 37.50%\nBatch 81/313 - Accuracy: 37.73%\nBatch 91/313 - Accuracy: 38.05%\nBatch 101/313 - Accuracy: 38.15%\nBatch 111/313 - Accuracy: 38.12%\nBatch 121/313 - Accuracy: 38.02%\nBatch 131/313 - Accuracy: 38.38%\nBatch 141/313 - Accuracy: 38.59%\nBatch 151/313 - Accuracy: 38.53%\nBatch 161/313 - Accuracy: 38.76%\nBatch 171/313 - Accuracy: 38.65%\nBatch 181/313 - Accuracy: 38.48%\nBatch 191/313 - Accuracy: 38.24%\nBatch 201/313 - Accuracy: 38.37%\nBatch 211/313 - Accuracy: 38.39%\nBatch 221/313 - Accuracy: 38.46%\nBatch 231/313 - Accuracy: 38.47%\nBatch 241/313 - Accuracy: 38.43%\nBatch 251/313 - Accuracy: 38.43%\nBatch 261/313 - Accuracy: 38.41%\nBatch 271/313 - Accuracy: 38.21%\nBatch 281/313 - Accuracy: 38.31%\nBatch 291/313 - Accuracy: 38.28%\nBatch 301/313 - Accuracy: 38.17%\nBatch 311/313 - Accuracy: 38.20%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 38.22%\nStudent (LogitMatching) accuracy on Texture-Bias = 38.22%\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"# Scrambled Dataset","metadata":{}},{"cell_type":"code","source":"# Path to the dataset and labels file\ndata_dir = '/kaggle/input/scrambled-dataset/images/'\nlabels_file = '/kaggle/input/scrambled-dataset/labels.txt'\n\n# Custom Dataset for Scrambled Dataset\nclass ScrambledDataset(Dataset):\n    def __init__(self, data_dir, labels_file, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        \n        # Read labels file\n        self.data = []\n        with open(labels_file, 'r') as f:\n            for line in f:\n                img_name, label = line.strip().split()\n                self.data.append((img_name, int(label)))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name, label = self.data[idx]\n        img_path = os.path.join(self.data_dir, img_name)\n        \n        # Load tensor image\n        image = torch.load(img_path)  # Load the .pt tensor file\n        \n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Define the transformation to resize tensors to 224x224\ndef tensor_transform(tensor):\n    # Ensure tensor is 4D (batch dimension needed for interpolate)\n    if tensor.dim() == 3:  # [C, H, W]\n        tensor = tensor.unsqueeze(0)  # Add batch dimension -> [1, C, H, W]\n    \n    # Resize to 224x224\n    resized_tensor = F.interpolate(tensor, size=(224, 224), mode='bilinear', align_corners=False)\n    \n    # Remove batch dimension if added\n    return resized_tensor.squeeze(0)  # Back to [C, H, W]\n\n# Create the dataset and DataLoader\nscrambled_dataset = ScrambledDataset(data_dir=data_dir, labels_file=labels_file, transform=tensor_transform)\nscrambled_loader = DataLoader(scrambled_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:28:07.243196Z","iopub.execute_input":"2024-12-25T17:28:07.243503Z","iopub.status.idle":"2024-12-25T17:28:07.271956Z","shell.execute_reply.started":"2024-12-25T17:28:07.243477Z","shell.execute_reply":"2024-12-25T17:28:07.270980Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Evaluating Logit matching student on Scrambled dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on srambled dataset\nscrambled_student_lm_acc = evaluate_model(student_lm, scrambled_loader, device)\nprint(f\"Student (LogitMatching) accuracy on Scrambled = {scrambled_student_lm_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:06:59.343642Z","iopub.execute_input":"2024-12-22T19:06:59.343927Z","iopub.status.idle":"2024-12-22T19:09:57.542780Z","shell.execute_reply.started":"2024-12-22T19:06:59.343906Z","shell.execute_reply":"2024-12-22T19:09:57.541997Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-36-bf889dc6a427>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 28.12%\nBatch 11/313 - Accuracy: 23.30%\nBatch 21/313 - Accuracy: 22.17%\nBatch 31/313 - Accuracy: 23.29%\nBatch 41/313 - Accuracy: 23.78%\nBatch 51/313 - Accuracy: 23.04%\nBatch 61/313 - Accuracy: 23.57%\nBatch 71/313 - Accuracy: 23.20%\nBatch 81/313 - Accuracy: 23.15%\nBatch 91/313 - Accuracy: 22.97%\nBatch 101/313 - Accuracy: 23.33%\nBatch 111/313 - Accuracy: 23.37%\nBatch 121/313 - Accuracy: 22.93%\nBatch 131/313 - Accuracy: 22.73%\nBatch 141/313 - Accuracy: 22.67%\nBatch 151/313 - Accuracy: 22.89%\nBatch 161/313 - Accuracy: 22.96%\nBatch 171/313 - Accuracy: 22.99%\nBatch 181/313 - Accuracy: 22.88%\nBatch 191/313 - Accuracy: 22.99%\nBatch 201/313 - Accuracy: 22.96%\nBatch 211/313 - Accuracy: 22.90%\nBatch 221/313 - Accuracy: 22.96%\nBatch 231/313 - Accuracy: 22.89%\nBatch 241/313 - Accuracy: 22.91%\nBatch 251/313 - Accuracy: 23.00%\nBatch 261/313 - Accuracy: 23.14%\nBatch 271/313 - Accuracy: 23.27%\nBatch 281/313 - Accuracy: 23.24%\nBatch 291/313 - Accuracy: 23.40%\nBatch 301/313 - Accuracy: 23.39%\nBatch 311/313 - Accuracy: 23.33%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 23.35%\nStudent (LogitMatching) accuracy on Texture-Bias = 23.35%\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# Noisy Dataset","metadata":{}},{"cell_type":"code","source":"# Path to the dataset and labels file\ndata_dir = '/kaggle/input/noisy-dataset/images/'\nlabels_file = '/kaggle/input/noisy-dataset/labels.txt'\n\n# Custom Dataset for Noisy Dataset\nclass NoisyDataset(Dataset):\n    def __init__(self, data_dir, labels_file, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        \n        # Read labels file\n        self.data = []\n        with open(labels_file, 'r') as f:\n            for line in f:\n                img_name, label = line.strip().split()\n                self.data.append((img_name, int(label)))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name, label = self.data[idx]\n        img_path = os.path.join(self.data_dir, img_name)\n        \n        # Load tensor image\n        image = torch.load(img_path)  # Load the .pt tensor file\n        \n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Define the transformation to resize tensors to 224x224\ndef tensor_transform(tensor):\n    # Ensure tensor is 4D (batch dimension needed for interpolate)\n    if tensor.dim() == 3:  # [C, H, W]\n        tensor = tensor.unsqueeze(0)  # Add batch dimension -> [1, C, H, W]\n    \n    # Resize to 224x224\n    resized_tensor = F.interpolate(tensor, size=(224, 224), mode='bilinear', align_corners=False)\n    \n    # Remove batch dimension if added\n    return resized_tensor.squeeze(0)  # Back to [C, H, W]\n\n# Create the dataset and DataLoader\nnoisy_dataset = NoisyDataset(data_dir=data_dir, labels_file=labels_file, transform=tensor_transform)\nnoisy_loader = DataLoader(noisy_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:28:12.571395Z","iopub.execute_input":"2024-12-25T17:28:12.571762Z","iopub.status.idle":"2024-12-25T17:28:12.603448Z","shell.execute_reply.started":"2024-12-25T17:28:12.571731Z","shell.execute_reply":"2024-12-25T17:28:12.602635Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Evaluating Logit matching student on Noisy dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on srambled dataset\nnoisy_student_lm_acc = evaluate_model(student_lm, noisy_loader, device)\nprint(f\"Student (LogitMatching) accuracy on Noisy = {noisy_student_lm_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:10:23.519279Z","iopub.execute_input":"2024-12-22T19:10:23.519572Z","iopub.status.idle":"2024-12-22T19:13:26.317598Z","shell.execute_reply.started":"2024-12-22T19:10:23.519549Z","shell.execute_reply":"2024-12-22T19:13:26.316603Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-38-f30d01fd9078>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 59.38%\nBatch 11/313 - Accuracy: 60.51%\nBatch 21/313 - Accuracy: 60.57%\nBatch 31/313 - Accuracy: 61.19%\nBatch 41/313 - Accuracy: 61.36%\nBatch 51/313 - Accuracy: 62.07%\nBatch 61/313 - Accuracy: 62.04%\nBatch 71/313 - Accuracy: 61.09%\nBatch 81/313 - Accuracy: 61.11%\nBatch 91/313 - Accuracy: 61.37%\nBatch 101/313 - Accuracy: 62.00%\nBatch 111/313 - Accuracy: 62.47%\nBatch 121/313 - Accuracy: 62.53%\nBatch 131/313 - Accuracy: 62.31%\nBatch 141/313 - Accuracy: 62.70%\nBatch 151/313 - Accuracy: 62.56%\nBatch 161/313 - Accuracy: 62.38%\nBatch 171/313 - Accuracy: 62.46%\nBatch 181/313 - Accuracy: 62.40%\nBatch 191/313 - Accuracy: 62.03%\nBatch 201/313 - Accuracy: 61.85%\nBatch 211/313 - Accuracy: 61.61%\nBatch 221/313 - Accuracy: 61.48%\nBatch 231/313 - Accuracy: 61.72%\nBatch 241/313 - Accuracy: 61.61%\nBatch 251/313 - Accuracy: 61.74%\nBatch 261/313 - Accuracy: 61.55%\nBatch 271/313 - Accuracy: 61.39%\nBatch 281/313 - Accuracy: 61.38%\nBatch 291/313 - Accuracy: 61.33%\nBatch 301/313 - Accuracy: 61.42%\nBatch 311/313 - Accuracy: 61.40%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 61.46%\nStudent (LogitMatching) accuracy on Texture-Bias = 61.46%\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"def get_features_and_logits(model, feature_model, inputs, student=True, crd=False):\n    if not student:\n        model.eval()\n        feature_model.eval()\n        features = feature_model(inputs)              # (B, 2048, 7, 7)\n        features = features.view(features.size(0), -1)  # (B, 100352)\n        return features, None\n    else:\n        model.train()\n        features = feature_model(inputs)               # if ViT also is truncated, might be (B, 768, 196)\n        features = features.view(features.size(0), -1) # (B, 150528)\n        logits = model(inputs)  # The student still does the full forward for classification\n        return features, logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:10:34.612140Z","iopub.execute_input":"2024-12-25T16:10:34.612476Z","iopub.status.idle":"2024-12-25T16:10:34.617572Z","shell.execute_reply.started":"2024-12-25T16:10:34.612453Z","shell.execute_reply":"2024-12-25T16:10:34.616671Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Knowledge Distillation (CRD)","metadata":{}},{"cell_type":"code","source":"# create CRD module\nnum_data = len(train_dataset)  # total CIFAR-10 training samples\ns_dim = get_channel_num(student_lm, \"vit\")         # 150528 or so\nt_dim = get_channel_num(teacher_model, \"resnet\")   # 2048\n\nprint(s_dim)\nprint(t_dim)\n\ncrd_module = CRD(\n    s_dim=s_dim, \n    t_dim=100352,\n    n_data=num_data,\n    feat_dim=128,    # or 256 etc., your choice\n    nce_n=4096,\n    nce_t=0.1,\n    nce_mom=0.5\n)\n\n# new student for CRD\nstudent_crd, features_crd = get_model('student')\nstudent_crd = student_crd.to(device)\n\n# Train with CRD\ntrain_student_crd(\n    student_crd, \n    features_crd, \n    teacher_model, \n    features_teacher, \n    train_crd_loader,  # your special loader with CIFAR10IdxSample\n    kd=crd_module, \n    num_epochs=5, \n    alpha=1e-3\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:46:07.536951Z","iopub.execute_input":"2024-12-22T19:46:07.537500Z","iopub.status.idle":"2024-12-22T22:11:48.337839Z","shell.execute_reply.started":"2024-12-22T19:46:07.537457Z","shell.execute_reply":"2024-12-22T22:11:48.336933Z"}},"outputs":[{"name":"stdout","text":"150528\n2048\nDistilling the knowledge and training the student on this data for CRD\nBatch 1/1563 - Loss: 21.9496 \nBatch 200/1563 - Loss: 19.6246 \nBatch 400/1563 - Loss: 19.5598 \nBatch 600/1563 - Loss: 19.6651 \nBatch 750/1563 - Loss: 19.6888 \nBatch 800/1563 - Loss: 19.4584 \nBatch 1000/1563 - Loss: 19.4957 \nBatch 1200/1563 - Loss: 20.0994 \nBatch 1400/1563 - Loss: 19.7063 \nEpoch [1/5], Loss: 19.7503\nBatch 1/1563 - Loss: 19.5297 \nBatch 200/1563 - Loss: 19.2098 \nBatch 400/1563 - Loss: 19.6271 \nBatch 600/1563 - Loss: 19.2578 \nBatch 750/1563 - Loss: 19.8765 \nBatch 800/1563 - Loss: 19.7548 \nBatch 1000/1563 - Loss: 19.4758 \nBatch 1200/1563 - Loss: 19.3368 \nBatch 1400/1563 - Loss: 19.6965 \nEpoch [2/5], Loss: 19.5663\nBatch 1/1563 - Loss: 19.3237 \nBatch 200/1563 - Loss: 19.5865 \nBatch 400/1563 - Loss: 19.5719 \nBatch 600/1563 - Loss: 19.4839 \nBatch 750/1563 - Loss: 19.4115 \nBatch 800/1563 - Loss: 18.8573 \nBatch 1000/1563 - Loss: 19.7717 \nBatch 1200/1563 - Loss: 19.1155 \nBatch 1400/1563 - Loss: 19.2722 \nEpoch [3/5], Loss: 19.5006\nBatch 1/1563 - Loss: 19.5306 \nBatch 200/1563 - Loss: 19.6435 \nBatch 400/1563 - Loss: 19.3593 \nBatch 600/1563 - Loss: 19.2442 \nBatch 750/1563 - Loss: 19.3670 \nBatch 800/1563 - Loss: 19.3996 \nBatch 1000/1563 - Loss: 19.9245 \nBatch 1200/1563 - Loss: 19.6226 \nBatch 1400/1563 - Loss: 19.5937 \nEpoch [4/5], Loss: 19.4744\nBatch 1/1563 - Loss: 19.5060 \nBatch 200/1563 - Loss: 19.2423 \nBatch 400/1563 - Loss: 19.5166 \nBatch 600/1563 - Loss: 19.5007 \nBatch 750/1563 - Loss: 19.3706 \nBatch 800/1563 - Loss: 19.2946 \nBatch 1000/1563 - Loss: 19.4835 \nBatch 1200/1563 - Loss: 19.5824 \nBatch 1400/1563 - Loss: 19.6281 \nEpoch [5/5], Loss: 19.4645\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"19.4645045262762"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"# Path to save the weights and features\nstudent_weights_path = \"student_crd_weights.pth\"\nteacher_weights_path = \"teacher_crd_weights.pth\"\nstudent_features_path = \"student_crd_features.pth\"\nteacher_features_path = \"teacher_crd_features.pth\"\n\n# Save the weights of the student and teacher models\ntorch.save(student_crd.state_dict(), student_weights_path)\ntorch.save(teacher_model.state_dict(), teacher_weights_path)\n\n# Save the features (if they are used later, e.g., for distillation or further training)\ntorch.save(features_crd, student_features_path)\ntorch.save(features_teacher, teacher_features_path)\n\nprint(\"Student and teacher weights and features saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:19:38.708171Z","iopub.execute_input":"2024-12-22T22:19:38.708700Z","iopub.status.idle":"2024-12-22T22:19:40.159241Z","shell.execute_reply.started":"2024-12-22T22:19:38.708657Z","shell.execute_reply":"2024-12-22T22:19:40.158297Z"}},"outputs":[{"name":"stdout","text":"Student and teacher weights and features saved successfully!\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"# Evaluating CRD student on Texture bias dataset","metadata":{}},{"cell_type":"code","source":"# ---------------------loadind crd student weights--------------------------------------\n# # Load the student model's weights\nstudent_crd, features_crd = get_model('student')  # Initialize the same architecture\nstudent_crd.load_state_dict(torch.load('/kaggle/input/contrastive-rd/student_crd_weights.pth'))\nstudent_crd = student_crd.to(device)\n\n# # Load the teacher model's weights\nteacher_model, features_teacher = get_model('teacher')  # Initialize the same architecture\nteacher_model.load_state_dict(torch.load('/kaggle/input/contrastive-rd/teacher_crd_weights.pth'))\nteacher_model = teacher_model.to(device)\n\n# # Load the features (if needed)\nfeatures_crd = torch.load('/kaggle/input/contrastive-rd/student_crd_features.pth')\nfeatures_teacher = torch.load('/kaggle/input/contrastive-rd/teacher_crd_features.pth')\n\nprint(\"Student and teacher weights and features loaded successfully!\")\n\n\n# Evaluate the CRD student on texture-bias\ntest_student_crd_acc = evaluate_model(student_crd, test_loader, device)\nprint(f\"Student (CRD) accuracy on Texture-Bias = {test_student_crd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:14:22.635510Z","iopub.execute_input":"2024-12-25T16:14:22.635841Z","iopub.status.idle":"2024-12-25T16:16:43.321274Z","shell.execute_reply.started":"2024-12-25T16:14:22.635818Z","shell.execute_reply":"2024-12-25T16:16:43.320332Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-19-ca64e6f299b6>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student_crd.load_state_dict(torch.load('/kaggle/input/contrastive-rd/student_crd_weights.pth'))\n<ipython-input-19-ca64e6f299b6>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  teacher_model.load_state_dict(torch.load('/kaggle/input/contrastive-rd/teacher_crd_weights.pth'))\n<ipython-input-19-ca64e6f299b6>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  features_crd = torch.load('/kaggle/input/contrastive-rd/student_crd_features.pth')\n<ipython-input-19-ca64e6f299b6>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  features_teacher = torch.load('/kaggle/input/contrastive-rd/teacher_crd_features.pth')\n","output_type":"stream"},{"name":"stdout","text":"Student and teacher weights and features loaded successfully!\nEvaluating the model\nBatch 1/313 - Accuracy: 100.00%\nBatch 11/313 - Accuracy: 94.89%\nBatch 21/313 - Accuracy: 95.68%\nBatch 31/313 - Accuracy: 96.37%\nBatch 41/313 - Accuracy: 95.96%\nBatch 51/313 - Accuracy: 95.89%\nBatch 61/313 - Accuracy: 95.54%\nBatch 71/313 - Accuracy: 95.47%\nBatch 81/313 - Accuracy: 95.37%\nBatch 91/313 - Accuracy: 95.19%\nBatch 101/313 - Accuracy: 95.14%\nBatch 111/313 - Accuracy: 95.05%\nBatch 121/313 - Accuracy: 95.02%\nBatch 131/313 - Accuracy: 95.04%\nBatch 141/313 - Accuracy: 95.12%\nBatch 151/313 - Accuracy: 95.03%\nBatch 161/313 - Accuracy: 94.97%\nBatch 171/313 - Accuracy: 95.05%\nBatch 181/313 - Accuracy: 95.10%\nBatch 191/313 - Accuracy: 95.04%\nBatch 201/313 - Accuracy: 95.07%\nBatch 211/313 - Accuracy: 95.10%\nBatch 221/313 - Accuracy: 95.14%\nBatch 231/313 - Accuracy: 95.21%\nBatch 241/313 - Accuracy: 95.16%\nBatch 251/313 - Accuracy: 95.16%\nBatch 261/313 - Accuracy: 95.15%\nBatch 271/313 - Accuracy: 95.17%\nBatch 281/313 - Accuracy: 95.16%\nBatch 291/313 - Accuracy: 95.14%\nBatch 301/313 - Accuracy: 95.16%\nBatch 311/313 - Accuracy: 95.21%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 95.21%\nStudent (CRD) accuracy on Texture-Bias = 95.21%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ---------------------loadind crd student weights--------------------------------------\n# # Load the student model's weights\n# student_crd, features_crd = get_model('student')  # Initialize the same architecture\n# student_crd.load_state_dict(torch.load(student_weights_path))\n# student_crd = student_crd.to(device)\n\n# # Load the teacher model's weights\n# teacher_model, features_teacher = get_model('teacher')  # Initialize the same architecture\n# teacher_model.load_state_dict(torch.load(teacher_weights_path))\n# teacher_model = teacher_model.to(device)\n\n# # Load the features (if needed)\n# features_crd = torch.load(student_features_path)\n# features_teacher = torch.load(teacher_features_path)\n\n# print(\"Student and teacher weights and features loaded successfully!\")\n\n\n# Evaluate the CRD student on texture-bias\ntexture_student_crd_acc = evaluate_model(student_crd, texture_bias_loader, device)\nprint(f\"Student (CRD) accuracy on Texture-Bias = {texture_student_crd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:20:16.576892Z","iopub.execute_input":"2024-12-22T22:20:16.577266Z","iopub.status.idle":"2024-12-22T22:22:20.902366Z","shell.execute_reply.started":"2024-12-22T22:20:16.577236Z","shell.execute_reply":"2024-12-22T22:22:20.901458Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 28.12%\nBatch 11/313 - Accuracy: 36.36%\nBatch 21/313 - Accuracy: 36.46%\nBatch 31/313 - Accuracy: 37.80%\nBatch 41/313 - Accuracy: 37.88%\nBatch 51/313 - Accuracy: 38.48%\nBatch 61/313 - Accuracy: 38.52%\nBatch 71/313 - Accuracy: 38.56%\nBatch 81/313 - Accuracy: 38.04%\nBatch 91/313 - Accuracy: 38.36%\nBatch 101/313 - Accuracy: 38.09%\nBatch 111/313 - Accuracy: 38.09%\nBatch 121/313 - Accuracy: 38.40%\nBatch 131/313 - Accuracy: 38.45%\nBatch 141/313 - Accuracy: 38.52%\nBatch 151/313 - Accuracy: 38.74%\nBatch 161/313 - Accuracy: 39.09%\nBatch 171/313 - Accuracy: 39.24%\nBatch 181/313 - Accuracy: 39.28%\nBatch 191/313 - Accuracy: 39.14%\nBatch 201/313 - Accuracy: 39.30%\nBatch 211/313 - Accuracy: 39.28%\nBatch 221/313 - Accuracy: 39.10%\nBatch 231/313 - Accuracy: 39.18%\nBatch 241/313 - Accuracy: 38.76%\nBatch 251/313 - Accuracy: 38.68%\nBatch 261/313 - Accuracy: 38.69%\nBatch 271/313 - Accuracy: 38.50%\nBatch 281/313 - Accuracy: 38.52%\nBatch 291/313 - Accuracy: 38.50%\nBatch 301/313 - Accuracy: 38.54%\nBatch 311/313 - Accuracy: 38.45%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 38.44%\nStudent (CRD) accuracy on Texture-Bias = 38.44%\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"# Evaluating CRD student on Shape bias dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on shape-bias\nshape_student_crd_acc = evaluate_model(student_crd, shape_bias_loader, device)\nprint(f\"Student (CRD) accuracy on Shape-Bias = {shape_student_crd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:22:20.903329Z","iopub.execute_input":"2024-12-22T22:22:20.903625Z","iopub.status.idle":"2024-12-22T22:24:55.171994Z","shell.execute_reply.started":"2024-12-22T22:22:20.903603Z","shell.execute_reply":"2024-12-22T22:24:55.171058Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 18.75%\nBatch 11/313 - Accuracy: 14.20%\nBatch 21/313 - Accuracy: 15.77%\nBatch 31/313 - Accuracy: 16.33%\nBatch 41/313 - Accuracy: 15.55%\nBatch 51/313 - Accuracy: 14.77%\nBatch 61/313 - Accuracy: 15.01%\nBatch 71/313 - Accuracy: 15.36%\nBatch 81/313 - Accuracy: 15.28%\nBatch 91/313 - Accuracy: 15.14%\nBatch 101/313 - Accuracy: 15.10%\nBatch 111/313 - Accuracy: 15.09%\nBatch 121/313 - Accuracy: 14.95%\nBatch 131/313 - Accuracy: 15.08%\nBatch 141/313 - Accuracy: 15.23%\nBatch 151/313 - Accuracy: 15.15%\nBatch 161/313 - Accuracy: 15.06%\nBatch 171/313 - Accuracy: 14.99%\nBatch 181/313 - Accuracy: 15.04%\nBatch 191/313 - Accuracy: 15.04%\nBatch 201/313 - Accuracy: 15.17%\nBatch 211/313 - Accuracy: 15.18%\nBatch 221/313 - Accuracy: 15.02%\nBatch 231/313 - Accuracy: 15.03%\nBatch 241/313 - Accuracy: 15.08%\nBatch 251/313 - Accuracy: 15.25%\nBatch 261/313 - Accuracy: 15.06%\nBatch 271/313 - Accuracy: 14.96%\nBatch 281/313 - Accuracy: 15.00%\nBatch 291/313 - Accuracy: 14.97%\nBatch 301/313 - Accuracy: 15.04%\nBatch 311/313 - Accuracy: 15.09%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 15.09%\nStudent (CRD) accuracy on Shape-Bias = 15.09%\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"# Evaluating CRD student on Scrambled dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on srambled dataset\nscrambled_student_crd_acc = evaluate_model(student_crd, scrambled_loader, device)\nprint(f\"Student (CRD) accuracy on Scrambled = {scrambled_student_crd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:24:55.173605Z","iopub.execute_input":"2024-12-22T22:24:55.173893Z","iopub.status.idle":"2024-12-22T22:27:22.507683Z","shell.execute_reply.started":"2024-12-22T22:24:55.173869Z","shell.execute_reply":"2024-12-22T22:27:22.506827Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-36-bf889dc6a427>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 28.12%\nBatch 11/313 - Accuracy: 22.16%\nBatch 21/313 - Accuracy: 20.68%\nBatch 31/313 - Accuracy: 20.77%\nBatch 41/313 - Accuracy: 20.81%\nBatch 51/313 - Accuracy: 20.16%\nBatch 61/313 - Accuracy: 20.24%\nBatch 71/313 - Accuracy: 20.29%\nBatch 81/313 - Accuracy: 19.87%\nBatch 91/313 - Accuracy: 19.71%\nBatch 101/313 - Accuracy: 19.96%\nBatch 111/313 - Accuracy: 19.71%\nBatch 121/313 - Accuracy: 19.55%\nBatch 131/313 - Accuracy: 19.56%\nBatch 141/313 - Accuracy: 19.86%\nBatch 151/313 - Accuracy: 20.18%\nBatch 161/313 - Accuracy: 20.13%\nBatch 171/313 - Accuracy: 20.21%\nBatch 181/313 - Accuracy: 20.27%\nBatch 191/313 - Accuracy: 20.37%\nBatch 201/313 - Accuracy: 20.26%\nBatch 211/313 - Accuracy: 20.47%\nBatch 221/313 - Accuracy: 20.57%\nBatch 231/313 - Accuracy: 20.45%\nBatch 241/313 - Accuracy: 20.46%\nBatch 251/313 - Accuracy: 20.49%\nBatch 261/313 - Accuracy: 20.61%\nBatch 271/313 - Accuracy: 20.64%\nBatch 281/313 - Accuracy: 20.64%\nBatch 291/313 - Accuracy: 20.68%\nBatch 301/313 - Accuracy: 20.64%\nBatch 311/313 - Accuracy: 20.55%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 20.52%\nStudent (CRD) accuracy on Scrambled = 20.52%\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"# Evaluating CRD student on Noised dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on noisy dataset\nnoisy_student_crd_acc = evaluate_model(student_crd, noisy_loader, device)\nprint(f\"Student (CRD) accuracy on Noisy = {noisy_student_crd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:27:22.508580Z","iopub.execute_input":"2024-12-22T22:27:22.508815Z","iopub.status.idle":"2024-12-22T22:29:48.934105Z","shell.execute_reply.started":"2024-12-22T22:27:22.508787Z","shell.execute_reply":"2024-12-22T22:29:48.933053Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-38-f30d01fd9078>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 65.62%\nBatch 11/313 - Accuracy: 67.61%\nBatch 21/313 - Accuracy: 68.15%\nBatch 31/313 - Accuracy: 67.14%\nBatch 41/313 - Accuracy: 67.68%\nBatch 51/313 - Accuracy: 67.89%\nBatch 61/313 - Accuracy: 67.78%\nBatch 71/313 - Accuracy: 66.42%\nBatch 81/313 - Accuracy: 66.28%\nBatch 91/313 - Accuracy: 66.59%\nBatch 101/313 - Accuracy: 67.14%\nBatch 111/313 - Accuracy: 67.15%\nBatch 121/313 - Accuracy: 67.41%\nBatch 131/313 - Accuracy: 66.98%\nBatch 141/313 - Accuracy: 67.33%\nBatch 151/313 - Accuracy: 67.18%\nBatch 161/313 - Accuracy: 67.02%\nBatch 171/313 - Accuracy: 67.20%\nBatch 181/313 - Accuracy: 67.16%\nBatch 191/313 - Accuracy: 67.03%\nBatch 201/313 - Accuracy: 66.95%\nBatch 211/313 - Accuracy: 66.91%\nBatch 221/313 - Accuracy: 66.78%\nBatch 231/313 - Accuracy: 66.90%\nBatch 241/313 - Accuracy: 66.78%\nBatch 251/313 - Accuracy: 66.92%\nBatch 261/313 - Accuracy: 66.71%\nBatch 271/313 - Accuracy: 66.57%\nBatch 281/313 - Accuracy: 66.65%\nBatch 291/313 - Accuracy: 66.52%\nBatch 301/313 - Accuracy: 66.52%\nBatch 311/313 - Accuracy: 66.52%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 66.56%\nStudent (CRD) accuracy on Noisy = 66.56%\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"# Evaluating CRD student on SuperPixelated dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on super pixelated dataset\nsuper_pixelated_student_crd_acc = evaluate_model(student_crd, superpixel_loader, device)\nprint(f\"Student (CRD) accuracy on Super Pixelated = {super_pixelated_student_crd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:29:48.935091Z","iopub.execute_input":"2024-12-22T22:29:48.935413Z","iopub.status.idle":"2024-12-22T22:32:44.170394Z","shell.execute_reply.started":"2024-12-22T22:29:48.935390Z","shell.execute_reply":"2024-12-22T22:32:44.169468Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 40.62%\nBatch 11/313 - Accuracy: 33.52%\nBatch 21/313 - Accuracy: 31.40%\nBatch 31/313 - Accuracy: 32.06%\nBatch 41/313 - Accuracy: 32.93%\nBatch 51/313 - Accuracy: 33.21%\nBatch 61/313 - Accuracy: 33.40%\nBatch 71/313 - Accuracy: 33.23%\nBatch 81/313 - Accuracy: 33.29%\nBatch 91/313 - Accuracy: 33.62%\nBatch 101/313 - Accuracy: 33.66%\nBatch 111/313 - Accuracy: 33.92%\nBatch 121/313 - Accuracy: 33.65%\nBatch 131/313 - Accuracy: 33.73%\nBatch 141/313 - Accuracy: 33.71%\nBatch 151/313 - Accuracy: 33.77%\nBatch 161/313 - Accuracy: 33.77%\nBatch 171/313 - Accuracy: 33.79%\nBatch 181/313 - Accuracy: 34.00%\nBatch 191/313 - Accuracy: 33.98%\nBatch 201/313 - Accuracy: 33.99%\nBatch 211/313 - Accuracy: 33.83%\nBatch 221/313 - Accuracy: 33.85%\nBatch 231/313 - Accuracy: 33.83%\nBatch 241/313 - Accuracy: 33.73%\nBatch 251/313 - Accuracy: 33.69%\nBatch 261/313 - Accuracy: 33.47%\nBatch 271/313 - Accuracy: 33.18%\nBatch 281/313 - Accuracy: 33.39%\nBatch 291/313 - Accuracy: 33.41%\nBatch 301/313 - Accuracy: 33.34%\nBatch 311/313 - Accuracy: 33.37%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 33.38%\nStudent (CRD) accuracy on Super Pixelated = 33.38%\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# Independent Student ","metadata":{}},{"cell_type":"code","source":"independent_student, features_ind_student = get_model('student')  # ViT\nprint(\"Training an independent student on CIFAR-10, no distillation\")\n\nindependent_student = independent_student.to(device)\n\n# no KD (kd=None)\nind_loss, ind_acc = finetune_model(\n    independent_student, \n    train_loader, \n    num_epochs=5, \n    alpha=1e-3\n)\n\n# Path to save the independent student model's weights and features\nindependent_student_weights_path = \"independent_student_weights.pth\"\nindependent_student_features_path = \"independent_student_features.pth\"\n\n# Save the weights of the independent student model\ntorch.save(independent_student.state_dict(), independent_student_weights_path)\n\n# Save the features (if they are computed or extracted during training)\ntorch.save(features_ind_student, independent_student_features_path)\n\nprint(\"Independent student weights and features saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:33:07.309092Z","iopub.execute_input":"2024-12-22T22:33:07.309413Z","iopub.status.idle":"2024-12-22T23:27:10.084585Z","shell.execute_reply.started":"2024-12-22T22:33:07.309391Z","shell.execute_reply":"2024-12-22T23:27:10.083320Z"}},"outputs":[{"name":"stdout","text":"Training an independent student on CIFAR-10, no distillation\nFinetuning the model on this data\nBatch 1/1563 - Loss: 2.7694 Accuracy: 9.38%\nBatch 301/1563 - Loss: 0.3283 Accuracy: 91.68%\nBatch 601/1563 - Loss: 0.0501 Accuracy: 93.11%\nBatch 751/1563 - Loss: 0.0556 Accuracy: 93.41%\nBatch 901/1563 - Loss: 0.0379 Accuracy: 93.79%\nBatch 1201/1563 - Loss: 0.0661 Accuracy: 94.16%\nBatch 1501/1563 - Loss: 0.2814 Accuracy: 94.34%\n------------------------->Epoch [1/5], Loss: 0.1775 Accuracy : 94.35%\nBatch 1/1563 - Loss: 0.3177 Accuracy: 90.62%\nBatch 301/1563 - Loss: 0.2355 Accuracy: 96.17%\nBatch 601/1563 - Loss: 0.0331 Accuracy: 96.08%\nBatch 751/1563 - Loss: 0.1093 Accuracy: 95.93%\nBatch 901/1563 - Loss: 0.1203 Accuracy: 95.88%\nBatch 1201/1563 - Loss: 0.0190 Accuracy: 95.88%\nBatch 1501/1563 - Loss: 0.0585 Accuracy: 95.84%\n------------------------->Epoch [2/5], Loss: 0.1268 Accuracy : 95.86%\nBatch 1/1563 - Loss: 0.1277 Accuracy: 96.88%\nBatch 301/1563 - Loss: 0.0892 Accuracy: 96.78%\nBatch 601/1563 - Loss: 0.0737 Accuracy: 96.28%\nBatch 751/1563 - Loss: 0.0772 Accuracy: 96.27%\nBatch 901/1563 - Loss: 0.0127 Accuracy: 96.32%\nBatch 1201/1563 - Loss: 0.0155 Accuracy: 96.24%\nBatch 1501/1563 - Loss: 0.1314 Accuracy: 96.18%\n------------------------->Epoch [3/5], Loss: 0.1134 Accuracy : 96.18%\nBatch 1/1563 - Loss: 0.0645 Accuracy: 96.88%\nBatch 301/1563 - Loss: 0.1174 Accuracy: 96.79%\nBatch 601/1563 - Loss: 0.0235 Accuracy: 96.64%\nBatch 751/1563 - Loss: 0.0141 Accuracy: 96.48%\nBatch 901/1563 - Loss: 0.0280 Accuracy: 96.47%\nBatch 1201/1563 - Loss: 0.1437 Accuracy: 96.41%\nBatch 1501/1563 - Loss: 0.0649 Accuracy: 96.40%\n------------------------->Epoch [4/5], Loss: 0.1055 Accuracy : 96.41%\nBatch 1/1563 - Loss: 0.2129 Accuracy: 96.88%\nBatch 301/1563 - Loss: 0.0031 Accuracy: 97.01%\nBatch 601/1563 - Loss: 0.2394 Accuracy: 96.97%\nBatch 751/1563 - Loss: 0.0507 Accuracy: 96.93%\nBatch 901/1563 - Loss: 0.0011 Accuracy: 96.93%\nBatch 1201/1563 - Loss: 0.0122 Accuracy: 96.76%\nBatch 1501/1563 - Loss: 0.1519 Accuracy: 96.68%\n------------------------->Epoch [5/5], Loss: 0.0996 Accuracy : 96.67%\nIndependent student weights and features saved successfully!\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"# Evaluating Independent Student on Texture Bias","metadata":{}},{"cell_type":"code","source":"# -------------------------loading weights of independent -----------------------------\n# Load the independent student's weights\nindependent_student, features_ind_student = get_model('student')  # Initialize the same architecture\nindependent_student.load_state_dict(torch.load('/kaggle/input/independent/independent_student_weights.pth'))\nindependent_student = independent_student.to(device)\n\n# # Load the features (if needed)\nfeatures_ind_student = torch.load('/kaggle/input/independent/independent_student_features.pth')\n\nprint(\"Independent student weights and features loaded successfully!\")\n\ntest_ind_acc = evaluate_model(independent_student, test_loader, device)\nprint(f\"Independent Student accuracy on Texture-Bias = {test_ind_acc*100:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:22:34.691351Z","iopub.execute_input":"2024-12-25T16:22:34.691731Z","iopub.status.idle":"2024-12-25T16:24:55.858656Z","shell.execute_reply.started":"2024-12-25T16:22:34.691700Z","shell.execute_reply":"2024-12-25T16:24:55.857813Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-20-0e901bc6e8d3>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  independent_student.load_state_dict(torch.load('/kaggle/input/independent/independent_student_weights.pth'))\n<ipython-input-20-0e901bc6e8d3>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  features_ind_student = torch.load('/kaggle/input/independent/independent_student_features.pth')\n","output_type":"stream"},{"name":"stdout","text":"Independent student weights and features loaded successfully!\nEvaluating the model\nBatch 1/313 - Accuracy: 100.00%\nBatch 11/313 - Accuracy: 95.74%\nBatch 21/313 - Accuracy: 95.98%\nBatch 31/313 - Accuracy: 96.37%\nBatch 41/313 - Accuracy: 95.88%\nBatch 51/313 - Accuracy: 95.40%\nBatch 61/313 - Accuracy: 95.59%\nBatch 71/313 - Accuracy: 95.51%\nBatch 81/313 - Accuracy: 95.49%\nBatch 91/313 - Accuracy: 95.54%\nBatch 101/313 - Accuracy: 95.36%\nBatch 111/313 - Accuracy: 95.35%\nBatch 121/313 - Accuracy: 95.43%\nBatch 131/313 - Accuracy: 95.42%\nBatch 141/313 - Accuracy: 95.52%\nBatch 151/313 - Accuracy: 95.45%\nBatch 161/313 - Accuracy: 95.44%\nBatch 171/313 - Accuracy: 95.41%\nBatch 181/313 - Accuracy: 95.46%\nBatch 191/313 - Accuracy: 95.44%\nBatch 201/313 - Accuracy: 95.43%\nBatch 211/313 - Accuracy: 95.47%\nBatch 221/313 - Accuracy: 95.49%\nBatch 231/313 - Accuracy: 95.55%\nBatch 241/313 - Accuracy: 95.47%\nBatch 251/313 - Accuracy: 95.44%\nBatch 261/313 - Accuracy: 95.40%\nBatch 271/313 - Accuracy: 95.39%\nBatch 281/313 - Accuracy: 95.36%\nBatch 291/313 - Accuracy: 95.33%\nBatch 301/313 - Accuracy: 95.36%\nBatch 311/313 - Accuracy: 95.39%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 95.39%\nIndependent Student accuracy on Texture-Bias = 95.39%\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# -------------------------loading weights of independent -----------------------------\n# Load the independent student's weights\n# independent_student, features_ind_student = get_model('student')  # Initialize the same architecture\n# independent_student.load_state_dict(torch.load(independent_student_weights_path))\n# independent_student = independent_student.to(device)\n\n# # Load the features (if needed)\n# features_ind_student = torch.load(independent_student_features_path)\n\n# print(\"Independent student weights and features loaded successfully!\")\n\n\ntexture_ind_acc = evaluate_model(independent_student, texture_bias_loader, device)\nprint(f\"Independent Student accuracy on Texture-Bias = {texture_ind_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T23:27:10.085837Z","iopub.execute_input":"2024-12-22T23:27:10.086080Z","iopub.status.idle":"2024-12-22T23:29:13.215123Z","shell.execute_reply.started":"2024-12-22T23:27:10.086059Z","shell.execute_reply":"2024-12-22T23:29:13.214302Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 21.88%\nBatch 11/313 - Accuracy: 34.94%\nBatch 21/313 - Accuracy: 34.97%\nBatch 31/313 - Accuracy: 37.30%\nBatch 41/313 - Accuracy: 37.42%\nBatch 51/313 - Accuracy: 37.44%\nBatch 61/313 - Accuracy: 37.60%\nBatch 71/313 - Accuracy: 37.76%\nBatch 81/313 - Accuracy: 37.46%\nBatch 91/313 - Accuracy: 37.60%\nBatch 101/313 - Accuracy: 37.56%\nBatch 111/313 - Accuracy: 37.67%\nBatch 121/313 - Accuracy: 38.09%\nBatch 131/313 - Accuracy: 38.24%\nBatch 141/313 - Accuracy: 38.39%\nBatch 151/313 - Accuracy: 38.64%\nBatch 161/313 - Accuracy: 38.98%\nBatch 171/313 - Accuracy: 39.00%\nBatch 181/313 - Accuracy: 39.21%\nBatch 191/313 - Accuracy: 39.28%\nBatch 201/313 - Accuracy: 39.52%\nBatch 211/313 - Accuracy: 39.35%\nBatch 221/313 - Accuracy: 39.17%\nBatch 231/313 - Accuracy: 39.37%\nBatch 241/313 - Accuracy: 39.04%\nBatch 251/313 - Accuracy: 39.03%\nBatch 261/313 - Accuracy: 38.98%\nBatch 271/313 - Accuracy: 38.84%\nBatch 281/313 - Accuracy: 38.87%\nBatch 291/313 - Accuracy: 38.73%\nBatch 301/313 - Accuracy: 38.76%\nBatch 311/313 - Accuracy: 38.60%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 38.58%\nIndependent Student accuracy on Texture-Bias = 38.58%\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"# Evaluating Independent student on Shape bias dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on shape-bias\nshape_student_ind_acc = evaluate_model(independent_student, shape_bias_loader, device)\nprint(f\"Student (Independent) accuracy on Shape-Bias = {shape_student_ind_acc*100:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T23:29:13.216593Z","iopub.execute_input":"2024-12-22T23:29:13.216848Z","iopub.status.idle":"2024-12-22T23:31:42.941721Z","shell.execute_reply.started":"2024-12-22T23:29:13.216827Z","shell.execute_reply":"2024-12-22T23:31:42.940761Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 9.38%\nBatch 11/313 - Accuracy: 13.35%\nBatch 21/313 - Accuracy: 15.18%\nBatch 31/313 - Accuracy: 16.63%\nBatch 41/313 - Accuracy: 15.70%\nBatch 51/313 - Accuracy: 15.56%\nBatch 61/313 - Accuracy: 15.68%\nBatch 71/313 - Accuracy: 16.07%\nBatch 81/313 - Accuracy: 16.20%\nBatch 91/313 - Accuracy: 16.21%\nBatch 101/313 - Accuracy: 16.15%\nBatch 111/313 - Accuracy: 16.08%\nBatch 121/313 - Accuracy: 15.78%\nBatch 131/313 - Accuracy: 16.17%\nBatch 141/313 - Accuracy: 16.31%\nBatch 151/313 - Accuracy: 16.20%\nBatch 161/313 - Accuracy: 16.34%\nBatch 171/313 - Accuracy: 16.37%\nBatch 181/313 - Accuracy: 16.49%\nBatch 191/313 - Accuracy: 16.44%\nBatch 201/313 - Accuracy: 16.64%\nBatch 211/313 - Accuracy: 16.72%\nBatch 221/313 - Accuracy: 16.56%\nBatch 231/313 - Accuracy: 16.67%\nBatch 241/313 - Accuracy: 16.69%\nBatch 251/313 - Accuracy: 16.80%\nBatch 261/313 - Accuracy: 16.62%\nBatch 271/313 - Accuracy: 16.55%\nBatch 281/313 - Accuracy: 16.61%\nBatch 291/313 - Accuracy: 16.65%\nBatch 301/313 - Accuracy: 16.67%\nBatch 311/313 - Accuracy: 16.77%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 16.76%\nStudent (Independent) accuracy on Shape-Bias = 16.76%\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"# Evaluating Independent student on Scrambled dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on scrambled dataset\nscrambled_student_ind_acc = evaluate_model(independent_student, scrambled_loader, device)\nprint(f\"Student (Independent) accuracy on Scrambled = {scrambled_student_ind_acc*100:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T23:31:42.942911Z","iopub.execute_input":"2024-12-22T23:31:42.943269Z","iopub.status.idle":"2024-12-22T23:34:10.227610Z","shell.execute_reply.started":"2024-12-22T23:31:42.943233Z","shell.execute_reply":"2024-12-22T23:34:10.226814Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-36-bf889dc6a427>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 34.38%\nBatch 11/313 - Accuracy: 23.58%\nBatch 21/313 - Accuracy: 21.73%\nBatch 31/313 - Accuracy: 21.77%\nBatch 41/313 - Accuracy: 22.03%\nBatch 51/313 - Accuracy: 21.32%\nBatch 61/313 - Accuracy: 21.52%\nBatch 71/313 - Accuracy: 21.39%\nBatch 81/313 - Accuracy: 21.22%\nBatch 91/313 - Accuracy: 21.15%\nBatch 101/313 - Accuracy: 21.50%\nBatch 111/313 - Accuracy: 21.65%\nBatch 121/313 - Accuracy: 21.46%\nBatch 131/313 - Accuracy: 21.18%\nBatch 141/313 - Accuracy: 21.43%\nBatch 151/313 - Accuracy: 21.61%\nBatch 161/313 - Accuracy: 21.53%\nBatch 171/313 - Accuracy: 21.71%\nBatch 181/313 - Accuracy: 21.74%\nBatch 191/313 - Accuracy: 21.81%\nBatch 201/313 - Accuracy: 21.80%\nBatch 211/313 - Accuracy: 21.95%\nBatch 221/313 - Accuracy: 21.97%\nBatch 231/313 - Accuracy: 21.88%\nBatch 241/313 - Accuracy: 22.00%\nBatch 251/313 - Accuracy: 22.01%\nBatch 261/313 - Accuracy: 22.09%\nBatch 271/313 - Accuracy: 22.06%\nBatch 281/313 - Accuracy: 22.06%\nBatch 291/313 - Accuracy: 21.98%\nBatch 301/313 - Accuracy: 21.99%\nBatch 311/313 - Accuracy: 21.95%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 21.95%\nStudent (Independent) accuracy on Scrambled = 21.95%\n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"# Evaluating Independent student on Noised dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on noisy dataset\nnoisy_student_ind_acc = evaluate_model(independent_student, noisy_loader, device)\nprint(f\"Student (Independent) accuracy on Noisy = {noisy_student_ind_acc*100:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T23:34:10.228460Z","iopub.execute_input":"2024-12-22T23:34:10.228694Z","iopub.status.idle":"2024-12-22T23:36:36.584101Z","shell.execute_reply.started":"2024-12-22T23:34:10.228673Z","shell.execute_reply":"2024-12-22T23:36:36.583324Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-38-f30d01fd9078>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 75.00%\nBatch 11/313 - Accuracy: 69.03%\nBatch 21/313 - Accuracy: 68.60%\nBatch 31/313 - Accuracy: 67.64%\nBatch 41/313 - Accuracy: 68.60%\nBatch 51/313 - Accuracy: 68.69%\nBatch 61/313 - Accuracy: 68.44%\nBatch 71/313 - Accuracy: 67.39%\nBatch 81/313 - Accuracy: 67.32%\nBatch 91/313 - Accuracy: 67.38%\nBatch 101/313 - Accuracy: 67.79%\nBatch 111/313 - Accuracy: 67.79%\nBatch 121/313 - Accuracy: 67.98%\nBatch 131/313 - Accuracy: 67.51%\nBatch 141/313 - Accuracy: 67.80%\nBatch 151/313 - Accuracy: 67.65%\nBatch 161/313 - Accuracy: 67.59%\nBatch 171/313 - Accuracy: 67.69%\nBatch 181/313 - Accuracy: 67.83%\nBatch 191/313 - Accuracy: 67.75%\nBatch 201/313 - Accuracy: 67.77%\nBatch 211/313 - Accuracy: 67.68%\nBatch 221/313 - Accuracy: 67.56%\nBatch 231/313 - Accuracy: 67.74%\nBatch 241/313 - Accuracy: 67.73%\nBatch 251/313 - Accuracy: 67.82%\nBatch 261/313 - Accuracy: 67.71%\nBatch 271/313 - Accuracy: 67.59%\nBatch 281/313 - Accuracy: 67.62%\nBatch 291/313 - Accuracy: 67.45%\nBatch 301/313 - Accuracy: 67.41%\nBatch 311/313 - Accuracy: 67.36%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 67.40%\nStudent (Independent) accuracy on Noisy = 67.40%\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"# Evaluating Independent student on SuperPixelated dataset","metadata":{}},{"cell_type":"code","source":"# Evaluate on super pixelated dataset\nsuper_pixelated_student_ind_acc = evaluate_model(independent_student, superpixel_loader, device)\nprint(f\"Student (Independent) accuracy on Super Pixelated = {super_pixelated_student_ind_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T23:36:36.584870Z","iopub.execute_input":"2024-12-22T23:36:36.585095Z","iopub.status.idle":"2024-12-22T23:39:27.776087Z","shell.execute_reply.started":"2024-12-22T23:36:36.585075Z","shell.execute_reply":"2024-12-22T23:39:27.774987Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 37.50%\nBatch 11/313 - Accuracy: 33.52%\nBatch 21/313 - Accuracy: 32.14%\nBatch 31/313 - Accuracy: 32.96%\nBatch 41/313 - Accuracy: 33.16%\nBatch 51/313 - Accuracy: 33.27%\nBatch 61/313 - Accuracy: 33.09%\nBatch 71/313 - Accuracy: 32.79%\nBatch 81/313 - Accuracy: 32.75%\nBatch 91/313 - Accuracy: 33.04%\nBatch 101/313 - Accuracy: 33.23%\nBatch 111/313 - Accuracy: 33.22%\nBatch 121/313 - Accuracy: 33.06%\nBatch 131/313 - Accuracy: 33.28%\nBatch 141/313 - Accuracy: 33.22%\nBatch 151/313 - Accuracy: 33.46%\nBatch 161/313 - Accuracy: 33.52%\nBatch 171/313 - Accuracy: 33.55%\nBatch 181/313 - Accuracy: 33.87%\nBatch 191/313 - Accuracy: 33.87%\nBatch 201/313 - Accuracy: 33.96%\nBatch 211/313 - Accuracy: 33.93%\nBatch 221/313 - Accuracy: 33.89%\nBatch 231/313 - Accuracy: 33.97%\nBatch 241/313 - Accuracy: 33.91%\nBatch 251/313 - Accuracy: 33.89%\nBatch 261/313 - Accuracy: 33.57%\nBatch 271/313 - Accuracy: 33.33%\nBatch 281/313 - Accuracy: 33.53%\nBatch 291/313 - Accuracy: 33.55%\nBatch 301/313 - Accuracy: 33.46%\nBatch 311/313 - Accuracy: 33.46%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 33.44%\nStudent (Independent) accuracy on Super Pixelated = 33.44%\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"# C2VKD (VLFD)","metadata":{}},{"cell_type":"code","source":"# Reload the teacher model\nteacher_model, features_teacher = get_model('teacher')  # Initialize the teacher architecture\nteacher_model.load_state_dict(torch.load('/kaggle/input/teacher-model-weights-and-features/teacher_model_weights.pth'))\nteacher_model = teacher_model.to(device)\nteacher_model.eval()  # Set to evaluation mode\n\n# Optionally load the features\nfeatures_teacher = torch.load('/kaggle/input/teacher-model-weights-and-features/features_teacher.pth')\n\nprint(\"Teacher model with features loaded successfully!\")\n\nstudent_c2vkd, features_c2vkd = get_model('student')  # The truncated ViT\nstudent_c2vkd = student_c2vkd.to(device)\n\n\n# Make sure this is placed at the global scope (e.g., near your other definitions).\n# This is crucial so we can project teacher's global pool (2048-d) to 768-d.\nteacher_global_proj = nn.Linear(2048, 768, bias=False).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:27:05.248055Z","iopub.execute_input":"2024-12-25T16:27:05.248396Z","iopub.status.idle":"2024-12-25T16:27:09.909722Z","shell.execute_reply.started":"2024-12-25T16:27:05.248366Z","shell.execute_reply":"2024-12-25T16:27:09.908800Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-21-96f0c2363058>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  teacher_model.load_state_dict(torch.load('/kaggle/input/teacher-model-weights-and-features/teacher_model_weights.pth'))\n<ipython-input-21-96f0c2363058>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  features_teacher = torch.load('/kaggle/input/teacher-model-weights-and-features/features_teacher.pth')\n","output_type":"stream"},{"name":"stdout","text":"Teacher model with features loaded successfully!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ===============================\n# FIXED C2VKD NOTEBOOK CELL \n# USING student_c2vkd.forward_features\n# ===============================\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n########################################\n# 1) TEACHER & STUDENT BACKBONE HELPERS\n########################################\n\n\ndef teacher_backbone_extract(images):\n    \"\"\"\n    Calls the truncated teacher backbone (features_teacher).\n    shape => (B, 2048, H, W) for ResNet-50 with layers removed.\n    \"\"\"\n    return features_teacher(images)\n\ndef student_backbone_extract_c2vkd(images):\n    \"\"\"\n    1) We call student_c2vkd.forward_features(images) to get patch embeddings \n       from the ViT (including CLS).\n    2) We remove the CLS token => shape (B, 196, 768).\n    3) Reshape => (B, 768, 14,14).\n\n    This depends on your timm-based ViT or a custom forward_features method.\n    \"\"\"\n    all_tokens = student_c2vkd.forward_features(images)  \n    patch_tokens = all_tokens[:, 1:, :]         # remove CLS => (B,196,768)\n    B, N, D = patch_tokens.shape\n    H = W = int(N**0.5)                         # 14\n    feats_4d = patch_tokens.reshape(B, H, W, D).permute(0, 3, 1, 2)  # (B,768,14,14)\n    return feats_4d\n\n########################################\n# 2) LOSSES FOR C2VKD (VLFD + PDD)\n########################################\n\ndef global_feature_loss(teacher_feats, student_feats):\n    \"\"\"\n    Teacher => (B,2048,Ht,Wt)\n    Student => (B,768,Hs,Ws)\n\n    We do global avg pooling, then \n    project teacher's 2048 -> 768 \n    so we can do KL on shape (B,768) vs. (B,768).\n    \"\"\"\n    # Teacher\n    B, Ct, Ht, Wt = teacher_feats.shape\n    teacher_global = F.adaptive_avg_pool2d(teacher_feats, (1,1)).view(B, Ct)  # (B, 2048)\n\n    # Student\n    B2, Cs, Hs, Ws = student_feats.shape\n    student_global = F.adaptive_avg_pool2d(student_feats, (1,1)).view(B2, Cs) # (B, 768)\n\n    # Project teacher => (B,768)\n    teacher_global_768 = teacher_global_proj(teacher_global)  # (B,768)\n\n    # Now do KL on (B,768) vs. (B,768)\n    return F.kl_div(\n        F.log_softmax(student_global, dim=1),\n        F.softmax(teacher_global_768, dim=1),\n        reduction='batchmean'\n    )\n\ndef patch_feature_loss(teacher_feats, student_feats):\n    \"\"\"\n    MSE alignment of teacher (B,2048,Ht,Wt) vs. student (B,768,Hs,Ws).\n    We resize teacher => (Hs,Ws). If channels differ (2048 vs. 768),\n    we do a linear projection too.\n    \"\"\"\n    B, Ct, Ht, Wt = teacher_feats.shape\n    B2, Cs, Hs, Ws = student_feats.shape\n\n    # Resize teacher to match student's (Hs,Ws)\n    teacher_resized = F.interpolate(teacher_feats, size=(Hs, Ws), mode='bilinear', align_corners=False)\n\n    if Ct != Cs:\n        # linear proj teacher from Ct->Cs\n        t_flat = teacher_resized.permute(0,2,3,1).reshape(-1, Ct)\n        linear_proj = nn.Linear(Ct, Cs, bias=False).to(teacher_feats.device)\n        t_proj = linear_proj(t_flat)     # => (B*Hs*Ws, Cs)\n        teacher_4d = t_proj.view(B, Hs, Ws, Cs).permute(0,3,1,2)  # => (B, Cs, Hs,Ws)\n        return F.mse_loss(teacher_4d, student_feats)\n    else:\n        return F.mse_loss(teacher_resized, student_feats)\n\nclass LinguisticAlignmentPatchBased(nn.Module):\n    \"\"\"\n    Projects a patch-level feature (B, N, C) -> (B, N, proj_dim) then normalizes.\n    \"\"\"\n    def __init__(self, in_dim, projection_dim):\n        super().__init__()\n        self.proj = nn.Linear(in_dim, projection_dim)\n\n    def forward(self, patch_feats):\n        # patch_feats: (B, N, in_dim)\n        out = self.proj(patch_feats)   # => (B, N, projection_dim)\n        out = F.normalize(out, dim=-1)\n        return out\n\ndef linguistic_feature_loss(\n    teacher_feats,   # (B, Ct, Ht, Wt)\n    student_feats,   # (B, Cs, Hs, Ws)\n    teacher_align_module,\n    student_align_module\n):\n    # 1) If teacher is 7x7 while student is 14x14, unify them\n    B, Ct, Ht, Wt = teacher_feats.shape\n    B2, Cs, Hs, Ws = student_feats.shape\n\n    if (Ht != Hs) or (Wt != Ws):\n        teacher_feats = F.interpolate(\n            teacher_feats, size=(Hs, Ws),\n            mode='bilinear', align_corners=False\n        )\n        # Now teacher_feats is (B, Ct, Hs, Ws)\n        # shape is consistent with student_feats\n\n    # 2) Flatten teacher => (B, Hs*Ws, Ct)\n    teacher_patches = teacher_feats.view(B, Ct, Hs*Ws).permute(0, 2, 1)\n\n    # 3) Flatten student => (B, Hs*Ws, Cs)\n    student_patches = student_feats.view(B2, Cs, Hs*Ws).permute(0, 2, 1)\n\n    # 4) Project both\n    teacher_proj = teacher_align_module(teacher_patches)   # => (B, Hs*Ws, proj_dim)\n    student_proj = student_align_module(student_patches)   # => (B, Hs*Ws, proj_dim)\n\n    # 5) Flatten to (B*N, proj_dim)\n    BN_teacher = teacher_proj.view(-1, teacher_proj.size(-1))\n    BN_student = student_proj.view(-1, student_proj.size(-1))\n\n    # 6) KL\n    return F.kl_div(\n        F.log_softmax(BN_student, dim=-1),\n        F.softmax(BN_teacher, dim=-1),\n        reduction='batchmean'\n    )\n\ndef pixel_wise_decoupled_loss(teacher_logits, student_logits, labels):\n    \"\"\"\n    For classification, teacher_logits & student_logits => (B, num_classes).\n    We separate teacher's target vs. non-target logit. Then do MSE vs. student's.\n    \"\"\"\n    B, num_classes = teacher_logits.shape\n    target_mask = F.one_hot(labels, num_classes).float() \n    non_target_mask = 1 - target_mask\n\n    t_target = (teacher_logits * target_mask).sum(dim=1, keepdim=True)\n    t_non    = (teacher_logits * non_target_mask).sum(dim=1, keepdim=True)\n    s_target = (student_logits * target_mask).sum(dim=1, keepdim=True)\n    s_non    = (student_logits * non_target_mask).sum(dim=1, keepdim=True)\n\n    return F.mse_loss(s_target, t_target) + F.mse_loss(s_non, t_non)\n\ndef total_distillation_loss(\n    teacher_feats, teacher_logits,\n    student_feats, student_logits,\n    labels,\n    teacher_align_module=None,\n    student_align_module=None\n):\n    \"\"\"\n    Summation of:\n      1) global feature loss (teacher->student dimension mismatch fixed)\n      2) patch feature loss\n      3) linguistic alignment (VLFD)\n      4) pixel decoupled (PDD)\n    \"\"\"\n    g_loss = global_feature_loss(teacher_feats, student_feats)\n    p_loss = patch_feature_loss(teacher_feats, student_feats)\n    ling_loss = 0\n    if teacher_align_module and student_align_module:\n        ling_loss = linguistic_feature_loss(teacher_feats, student_feats, \n                                            teacher_align_module, student_align_module)\n    pdd_loss = pixel_wise_decoupled_loss(teacher_logits, student_logits, labels)\n\n    return 0.5*g_loss + 0.3*p_loss + 0.2*ling_loss + pdd_loss\n\n\n########################################\n# 3) C2VKD TRAIN LOOP\n########################################\ndef train_student_c2vkd(\n    student_model, teacher_model, \n    train_loader, optimizer,\n    teacher_align_module=None, student_align_module=None,\n    num_epochs=10, device='cuda'\n):\n    \"\"\"\n    Classification-based distillation (C2VKD).\n    Expects teacher_model => full ResNet for teacher logits,\n    teacher_backbone_extract => truncated teacher for 4D feats,\n    student_model => ViT for logits,\n    student_backbone_extract_c2vkd => 4D patch embeddings from the student's forward_features.\n    \"\"\"\n    student_model.train()\n    teacher_model.eval()\n\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        print(f\"Starting Epoch {epoch + 1}/{num_epochs}...\")\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Teacher forward\n            with torch.no_grad():\n                teacher_logits = teacher_model(images)           # (B, #classes)\n                teacher_feats  = teacher_backbone_extract(images) # (B,2048,Ht,Wt)\n\n            # Student forward\n            student_logits = student_model(images)               # (B, #classes)\n            student_feats  = student_backbone_extract_c2vkd(images) # (B,768,Hs,Ws)\n\n            # Compute total distillation\n            loss = total_distillation_loss(\n                teacher_feats, teacher_logits,\n                student_feats, student_logits,\n                labels,\n                teacher_align_module, \n                student_align_module\n            )\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"[Epoch {epoch+1}/{num_epochs}] => Loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:28:34.068876Z","iopub.execute_input":"2024-12-25T16:28:34.069172Z","iopub.status.idle":"2024-12-25T16:28:34.086297Z","shell.execute_reply.started":"2024-12-25T16:28:34.069148Z","shell.execute_reply":"2024-12-25T16:28:34.085347Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# from torch.utils.data import Subset, DataLoader\n# import numpy as np\n\n# # Limit the dataset to 1000 samples\n# subset_indices = np.random.choice(len(train_dataset), 1000, replace=False)  # Randomly select 1000 indices\n# subset_train_dataset = Subset(train_dataset, subset_indices)\n\n# # Create a DataLoader for the subset\n# subset_train_loader = DataLoader(\n#     subset_train_dataset, \n#     batch_size=32,  # Adjust batch size as needed\n#     shuffle=True, \n#     num_workers=4  # Adjust num_workers based on your system\n# )\n\n# # Define alignment modules\n# teacher_align_module = LinguisticAlignmentPatchBased(in_dim=2048, projection_dim=512).to(device)\n# student_align_module = LinguisticAlignmentPatchBased(in_dim=768, projection_dim=512).to(device)\n\n# # Optimizer\n# optimizer = optim.Adam(student_c2vkd.parameters(), lr=1e-3)\n\n# # Run training on the small subset\n# train_student_c2vkd(\n#     student_model=student_c2vkd,\n#     teacher_model=teacher_model,\n#     train_loader=subset_train_loader,\n#     optimizer=optimizer,\n#     teacher_align_module=teacher_align_module,\n#     student_align_module=student_align_module,\n#     num_epochs=10,  # Keep fewer epochs for quick testing\n#     device=device\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T06:41:42.881920Z","iopub.execute_input":"2024-12-24T06:41:42.882257Z","iopub.status.idle":"2024-12-24T06:41:42.886246Z","shell.execute_reply.started":"2024-12-24T06:41:42.882230Z","shell.execute_reply":"2024-12-24T06:41:42.885135Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"########################################\n# 4) EXAMPLE USAGE\n########################################\n\n# We assume you already have:\n#  teacher_model, features_teacher = get_model('teacher')\n#  student_c2vkd, features_c2vkd  = get_model('student')\n#  teacher_model is loaded with fine-tuned weights\n#  The \"features_teacher\" and \"features_c2vkd\" are your truncated networks\n#  We do timm's forward_features => student_c2vkd.forward_features(...) inside our helper\n\n# Create alignment modules\nteacher_align_module = LinguisticAlignmentPatchBased(in_dim=2048, projection_dim=512).to(device)\nstudent_align_module = LinguisticAlignmentPatchBased(in_dim=768,  projection_dim=512).to(device)\n\noptimizer = optim.Adam(student_c2vkd.parameters(), lr=1e-3)\n\ntrain_student_c2vkd(\n    student_model=student_c2vkd,\n    teacher_model=teacher_model,\n    train_loader=train_loader,\n    optimizer=optimizer,\n    teacher_align_module=teacher_align_module,\n    student_align_module=student_align_module,\n    num_epochs=5,\n    device=device\n)\n\n\n# Save the student's weights\ntorch.save(student_c2vkd.state_dict(), 'student_c2vkd_model_weights.pth')\n\n# Save the student's features (if needed for truncated forward)\ntorch.save(features_c2vkd.state_dict(), 'student_c2vkd_features.pth')\n\nprint(\"Student model weights and features saved successfully!\")\n\n\n\n# Evaluate on texture, shape, etc.\n# evaluate_model(student_c2vkd, texture_bias_loader, device)\n# ...\n\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T06:41:47.393547Z","iopub.execute_input":"2024-12-24T06:41:47.393989Z","iopub.status.idle":"2024-12-24T08:55:29.061602Z","shell.execute_reply.started":"2024-12-24T06:41:47.393952Z","shell.execute_reply":"2024-12-24T08:55:29.060847Z"}},"outputs":[{"name":"stdout","text":"Starting Epoch 1/5...\n[Epoch 1/5] => Loss: 59.3676\nStarting Epoch 2/5...\n[Epoch 2/5] => Loss: 39.3139\nStarting Epoch 3/5...\n[Epoch 3/5] => Loss: 37.9903\nStarting Epoch 4/5...\n[Epoch 4/5] => Loss: 36.8878\nStarting Epoch 5/5...\n[Epoch 5/5] => Loss: 36.3656\nStudent model weights and features saved successfully!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# need to save this student and it's features too\n# then load and evaluate of texture bias and then so on\n\n# Initialize the student model and features\nstudent_c2vkd, features_c2vkd = get_model('student')  # Ensure same architecture as saved model\nstudent_c2vkd = student_c2vkd.to(device)\n\n# # Load the student's weights\nstudent_c2vkd.load_state_dict(torch.load('/kaggle/input/ctovkd/student_c2vkd_model_weights.pth'))\nstudent_c2vkd.eval()  # Set to evaluation mode\n\n# # Optionally, load the features\nfeatures_c2vkd.load_state_dict(torch.load('/kaggle/input/ctovkd/student_c2vkd_features.pth'))\n\nprint(\"Student model weights and features loaded successfully!\")\n\ntest_c2vkd_acc = evaluate_model(student_c2vkd, test_loader, device)\nprint(f\"Student (C2VKD) accuracy on Texture-Bias = {test_c2vkd_acc*100:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:30:47.077698Z","iopub.execute_input":"2024-12-25T16:30:47.078017Z","iopub.status.idle":"2024-12-25T16:33:06.688560Z","shell.execute_reply.started":"2024-12-25T16:30:47.077990Z","shell.execute_reply":"2024-12-25T16:33:06.687731Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-24-354538667617>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student_c2vkd.load_state_dict(torch.load('/kaggle/input/ctovkd/student_c2vkd_model_weights.pth'))\n<ipython-input-24-354538667617>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  features_c2vkd.load_state_dict(torch.load('/kaggle/input/ctovkd/student_c2vkd_features.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Student model weights and features loaded successfully!\nEvaluating the model\nBatch 1/313 - Accuracy: 96.88%\nBatch 11/313 - Accuracy: 88.64%\nBatch 21/313 - Accuracy: 88.84%\nBatch 31/313 - Accuracy: 88.31%\nBatch 41/313 - Accuracy: 88.11%\nBatch 51/313 - Accuracy: 87.87%\nBatch 61/313 - Accuracy: 88.01%\nBatch 71/313 - Accuracy: 87.90%\nBatch 81/313 - Accuracy: 87.81%\nBatch 91/313 - Accuracy: 87.71%\nBatch 101/313 - Accuracy: 87.62%\nBatch 111/313 - Accuracy: 87.73%\nBatch 121/313 - Accuracy: 87.63%\nBatch 131/313 - Accuracy: 87.64%\nBatch 141/313 - Accuracy: 87.94%\nBatch 151/313 - Accuracy: 87.85%\nBatch 161/313 - Accuracy: 87.99%\nBatch 171/313 - Accuracy: 88.03%\nBatch 181/313 - Accuracy: 87.97%\nBatch 191/313 - Accuracy: 87.94%\nBatch 201/313 - Accuracy: 88.08%\nBatch 211/313 - Accuracy: 87.90%\nBatch 221/313 - Accuracy: 87.92%\nBatch 231/313 - Accuracy: 88.15%\nBatch 241/313 - Accuracy: 88.14%\nBatch 251/313 - Accuracy: 88.07%\nBatch 261/313 - Accuracy: 88.04%\nBatch 271/313 - Accuracy: 87.97%\nBatch 281/313 - Accuracy: 87.99%\nBatch 291/313 - Accuracy: 87.97%\nBatch 301/313 - Accuracy: 88.08%\nBatch 311/313 - Accuracy: 88.15%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 88.13%\nStudent (C2VKD) accuracy on Texture-Bias = 88.13%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# need to save this student and it's features too\n# then load and evaluate of texture bias and then so on\n\n# Initialize the student model and features\n# student_c2vkd, features_c2vkd = get_model('student')  # Ensure same architecture as saved model\n# student_c2vkd = student_c2vkd.to(device)\n\n# # Load the student's weights\n# student_c2vkd.load_state_dict(torch.load('student_c2vkd_model_weights.pth'))\n# student_c2vkd.eval()  # Set to evaluation mode\n\n# # Optionally, load the features\n# features_c2vkd.load_state_dict(torch.load('student_c2vkd_features.pth'))\n\n# print(\"Student model weights and features loaded successfully!\")\n\n\n# Evaluate on texture-bias dataset\ntexture_c2vkd_acc = evaluate_model(student_c2vkd, texture_bias_loader, device)\nprint(f\"Student (C2VKD) accuracy on Texture-Bias = {texture_c2vkd_acc*100:.2f}%\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T08:56:44.340993Z","iopub.execute_input":"2024-12-24T08:56:44.341310Z","iopub.status.idle":"2024-12-24T08:58:33.809141Z","shell.execute_reply.started":"2024-12-24T08:56:44.341287Z","shell.execute_reply":"2024-12-24T08:58:33.808360Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 46.88%\nBatch 11/313 - Accuracy: 32.10%\nBatch 21/313 - Accuracy: 30.36%\nBatch 31/313 - Accuracy: 30.54%\nBatch 41/313 - Accuracy: 30.72%\nBatch 51/313 - Accuracy: 30.45%\nBatch 61/313 - Accuracy: 30.28%\nBatch 71/313 - Accuracy: 30.68%\nBatch 81/313 - Accuracy: 30.63%\nBatch 91/313 - Accuracy: 30.80%\nBatch 101/313 - Accuracy: 30.41%\nBatch 111/313 - Accuracy: 30.63%\nBatch 121/313 - Accuracy: 31.12%\nBatch 131/313 - Accuracy: 31.25%\nBatch 141/313 - Accuracy: 31.43%\nBatch 151/313 - Accuracy: 31.52%\nBatch 161/313 - Accuracy: 31.44%\nBatch 171/313 - Accuracy: 31.63%\nBatch 181/313 - Accuracy: 31.68%\nBatch 191/313 - Accuracy: 31.48%\nBatch 201/313 - Accuracy: 31.39%\nBatch 211/313 - Accuracy: 31.26%\nBatch 221/313 - Accuracy: 31.17%\nBatch 231/313 - Accuracy: 31.29%\nBatch 241/313 - Accuracy: 31.11%\nBatch 251/313 - Accuracy: 31.09%\nBatch 261/313 - Accuracy: 31.06%\nBatch 271/313 - Accuracy: 30.90%\nBatch 281/313 - Accuracy: 30.95%\nBatch 291/313 - Accuracy: 30.91%\nBatch 301/313 - Accuracy: 30.88%\nBatch 311/313 - Accuracy: 30.76%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 30.78%\nStudent (C2VKD) accuracy on Texture-Bias = 30.78%\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Evaluate on shape-bias dataset\nshape_c2vkd_acc = evaluate_model(student_c2vkd, shape_bias_loader, device)\nprint(f\"Student (C2VKD) accuracy on Shape-Bias = {shape_c2vkd_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T08:58:33.810322Z","iopub.execute_input":"2024-12-24T08:58:33.810644Z","iopub.status.idle":"2024-12-24T09:01:55.377066Z","shell.execute_reply.started":"2024-12-24T08:58:33.810617Z","shell.execute_reply":"2024-12-24T09:01:55.376221Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 9.38%\nBatch 11/313 - Accuracy: 15.06%\nBatch 21/313 - Accuracy: 15.33%\nBatch 31/313 - Accuracy: 15.93%\nBatch 41/313 - Accuracy: 16.69%\nBatch 51/313 - Accuracy: 16.48%\nBatch 61/313 - Accuracy: 16.19%\nBatch 71/313 - Accuracy: 16.33%\nBatch 81/313 - Accuracy: 16.20%\nBatch 91/313 - Accuracy: 16.14%\nBatch 101/313 - Accuracy: 16.09%\nBatch 111/313 - Accuracy: 15.68%\nBatch 121/313 - Accuracy: 15.39%\nBatch 131/313 - Accuracy: 15.31%\nBatch 141/313 - Accuracy: 15.38%\nBatch 151/313 - Accuracy: 15.62%\nBatch 161/313 - Accuracy: 15.95%\nBatch 171/313 - Accuracy: 15.94%\nBatch 181/313 - Accuracy: 15.80%\nBatch 191/313 - Accuracy: 15.77%\nBatch 201/313 - Accuracy: 15.70%\nBatch 211/313 - Accuracy: 15.68%\nBatch 221/313 - Accuracy: 15.77%\nBatch 231/313 - Accuracy: 15.69%\nBatch 241/313 - Accuracy: 15.64%\nBatch 251/313 - Accuracy: 15.70%\nBatch 261/313 - Accuracy: 15.72%\nBatch 271/313 - Accuracy: 15.82%\nBatch 281/313 - Accuracy: 15.87%\nBatch 291/313 - Accuracy: 15.85%\nBatch 301/313 - Accuracy: 15.87%\nBatch 311/313 - Accuracy: 15.93%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 15.92%\nStudent (C2VKD) accuracy on Shape-Bias = 15.92%\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Evaluate on scrambled dataset\nscrambled_c2vkd_acc = evaluate_model(student_c2vkd, scrambled_loader, device)\nprint(f\"Student (C2VKD) accuracy on Scrambled = {scrambled_c2vkd_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T09:01:55.378612Z","iopub.execute_input":"2024-12-24T09:01:55.378917Z","iopub.status.idle":"2024-12-24T09:05:08.181545Z","shell.execute_reply.started":"2024-12-24T09:01:55.378889Z","shell.execute_reply":"2024-12-24T09:05:08.180760Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-16-bf889dc6a427>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 31.25%\nBatch 11/313 - Accuracy: 23.58%\nBatch 21/313 - Accuracy: 23.66%\nBatch 31/313 - Accuracy: 23.89%\nBatch 41/313 - Accuracy: 24.16%\nBatch 51/313 - Accuracy: 23.28%\nBatch 61/313 - Accuracy: 24.03%\nBatch 71/313 - Accuracy: 24.08%\nBatch 81/313 - Accuracy: 23.65%\nBatch 91/313 - Accuracy: 23.87%\nBatch 101/313 - Accuracy: 23.92%\nBatch 111/313 - Accuracy: 24.16%\nBatch 121/313 - Accuracy: 23.76%\nBatch 131/313 - Accuracy: 23.78%\nBatch 141/313 - Accuracy: 23.76%\nBatch 151/313 - Accuracy: 23.97%\nBatch 161/313 - Accuracy: 24.11%\nBatch 171/313 - Accuracy: 23.99%\nBatch 181/313 - Accuracy: 24.02%\nBatch 191/313 - Accuracy: 24.03%\nBatch 201/313 - Accuracy: 23.96%\nBatch 211/313 - Accuracy: 23.92%\nBatch 221/313 - Accuracy: 24.05%\nBatch 231/313 - Accuracy: 24.07%\nBatch 241/313 - Accuracy: 24.08%\nBatch 251/313 - Accuracy: 24.17%\nBatch 261/313 - Accuracy: 24.26%\nBatch 271/313 - Accuracy: 24.19%\nBatch 281/313 - Accuracy: 24.34%\nBatch 291/313 - Accuracy: 24.45%\nBatch 301/313 - Accuracy: 24.46%\nBatch 311/313 - Accuracy: 24.43%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 24.41%\nStudent (C2VKD) accuracy on Scrambled = 24.41%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Evaluate on noisy dataset\nnoisy_c2vkd_acc = evaluate_model(student_c2vkd, noisy_loader, device)\nprint(f\"Student (C2VKD) accuracy on Noisy = {noisy_c2vkd_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T09:05:08.182601Z","iopub.execute_input":"2024-12-24T09:05:08.183103Z","iopub.status.idle":"2024-12-24T09:08:25.103848Z","shell.execute_reply.started":"2024-12-24T09:05:08.183072Z","shell.execute_reply":"2024-12-24T09:08:25.102903Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-17-f30d01fd9078>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 59.38%\nBatch 11/313 - Accuracy: 53.98%\nBatch 21/313 - Accuracy: 55.80%\nBatch 31/313 - Accuracy: 56.05%\nBatch 41/313 - Accuracy: 55.64%\nBatch 51/313 - Accuracy: 56.50%\nBatch 61/313 - Accuracy: 57.02%\nBatch 71/313 - Accuracy: 55.55%\nBatch 81/313 - Accuracy: 55.44%\nBatch 91/313 - Accuracy: 55.87%\nBatch 101/313 - Accuracy: 56.50%\nBatch 111/313 - Accuracy: 56.98%\nBatch 121/313 - Accuracy: 57.33%\nBatch 131/313 - Accuracy: 57.16%\nBatch 141/313 - Accuracy: 57.71%\nBatch 151/313 - Accuracy: 57.35%\nBatch 161/313 - Accuracy: 57.10%\nBatch 171/313 - Accuracy: 57.13%\nBatch 181/313 - Accuracy: 57.08%\nBatch 191/313 - Accuracy: 56.87%\nBatch 201/313 - Accuracy: 56.72%\nBatch 211/313 - Accuracy: 56.69%\nBatch 221/313 - Accuracy: 56.72%\nBatch 231/313 - Accuracy: 56.78%\nBatch 241/313 - Accuracy: 56.78%\nBatch 251/313 - Accuracy: 56.80%\nBatch 261/313 - Accuracy: 56.60%\nBatch 271/313 - Accuracy: 56.58%\nBatch 281/313 - Accuracy: 56.62%\nBatch 291/313 - Accuracy: 56.57%\nBatch 301/313 - Accuracy: 56.54%\nBatch 311/313 - Accuracy: 56.40%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 56.45%\nStudent (C2VKD) accuracy on Noisy = 56.45%\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Evaluate on super-pixelated dataset\nsuper_pixelated_c2vkd_acc = evaluate_model(student_c2vkd, superpixel_loader, device)\nprint(f\"Student (C2VKD) accuracy on Super Pixelated = {super_pixelated_c2vkd_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T09:08:25.104781Z","iopub.execute_input":"2024-12-24T09:08:25.105107Z","iopub.status.idle":"2024-12-24T09:12:27.712129Z","shell.execute_reply.started":"2024-12-24T09:08:25.105075Z","shell.execute_reply":"2024-12-24T09:12:27.711257Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 40.62%\nBatch 11/313 - Accuracy: 30.97%\nBatch 21/313 - Accuracy: 30.80%\nBatch 31/313 - Accuracy: 29.64%\nBatch 41/313 - Accuracy: 30.87%\nBatch 51/313 - Accuracy: 30.21%\nBatch 61/313 - Accuracy: 29.76%\nBatch 71/313 - Accuracy: 29.97%\nBatch 81/313 - Accuracy: 29.90%\nBatch 91/313 - Accuracy: 30.73%\nBatch 101/313 - Accuracy: 30.66%\nBatch 111/313 - Accuracy: 30.63%\nBatch 121/313 - Accuracy: 30.91%\nBatch 131/313 - Accuracy: 31.01%\nBatch 141/313 - Accuracy: 31.29%\nBatch 151/313 - Accuracy: 31.39%\nBatch 161/313 - Accuracy: 31.56%\nBatch 171/313 - Accuracy: 31.69%\nBatch 181/313 - Accuracy: 31.66%\nBatch 191/313 - Accuracy: 31.82%\nBatch 201/313 - Accuracy: 31.78%\nBatch 211/313 - Accuracy: 31.68%\nBatch 221/313 - Accuracy: 31.70%\nBatch 231/313 - Accuracy: 31.76%\nBatch 241/313 - Accuracy: 31.66%\nBatch 251/313 - Accuracy: 31.72%\nBatch 261/313 - Accuracy: 31.56%\nBatch 271/313 - Accuracy: 31.52%\nBatch 281/313 - Accuracy: 31.53%\nBatch 291/313 - Accuracy: 31.59%\nBatch 301/313 - Accuracy: 31.58%\nBatch 311/313 - Accuracy: 31.60%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 31.58%\nStudent (C2VKD) accuracy on Super Pixelated = 31.58%\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# CSKD","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass ViTWithDenseOutputs(nn.Module):\n    def __init__(self, base_model, num_classes=10):\n        super(ViTWithDenseOutputs, self).__init__()\n        self.base_model = base_model\n        self.head = nn.Linear(base_model.head.in_features, num_classes)\n        self.dense_head = nn.Linear(base_model.head.in_features, num_classes)\n\n    def forward(self, x):\n        # Get all tokens ([CLS] + patch tokens)\n        all_tokens = self.base_model.forward_features(x)  \n        # For vit_base_patch16_224, shape = [B, 197, C] if no distillation\n\n        # The first token is CLS, the rest are patch tokens:\n        cls_token = all_tokens[:, 0, :]      # shape [B, C]\n        patch_tokens = all_tokens[:, 1:, :]  # shape [B, 196, C]\n\n        # Global logits from CLS\n        global_logits = self.head(cls_token)\n\n        # Dense logits from patch tokens\n        dense_logits = self.dense_head(patch_tokens)  # shape [B, 196, num_classes]\n\n        return global_logits, global_logits, dense_logits\n\n\n\n\ndef create_vit_model(num_classes=10):\n    \"\"\"Create a ViT model with a dynamic number of output classes.\"\"\"\n    base_model = create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n\n    # Freeze all layers in the ViT model\n    for param in base_model.parameters():\n        param.requires_grad = False\n\n    # Replace the classifier head with a custom one\n    base_model.head = torch.nn.Linear(base_model.head.in_features, num_classes)\n\n    # Wrap the model to include dense outputs\n    vit_model = ViTWithDenseOutputs(base_model, num_classes=num_classes)\n    return vit_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:35:04.157221Z","iopub.execute_input":"2024-12-25T16:35:04.157576Z","iopub.status.idle":"2024-12-25T16:35:04.164080Z","shell.execute_reply.started":"2024-12-25T16:35:04.157550Z","shell.execute_reply":"2024-12-25T16:35:04.163300Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CSKDLoss(nn.Module):\n    \"\"\"\n    Cumulative Spatial Knowledge Distillation Loss\n    \"\"\"\n    def __init__(self, cfg, criterion, teacher):\n        super(CSKDLoss, self).__init__()\n        self.cfg = cfg  # Configuration object\n        self.criterion = criterion  # Base criterion (e.g., CrossEntropyLoss)\n        self.teacher = teacher  # Teacher model\n\n    def forward(self, inputs, outputs, labels, epoch, max_epoch):\n        # If the model outputs multiple tensors (e.g., logits and dense logits)\n        if not isinstance(outputs, torch.Tensor):\n            outputs, stu_deit_logits, stu_dense_logits = outputs\n        else:\n            raise ValueError(\"Expected multiple outputs including dense logits from the student model.\")\n\n        # Ensure global logits match the teacher's logits in shape\n        stu_deit_logits = F.log_softmax(stu_deit_logits, dim=-1)\n\n        # Base classification loss\n        loss_base = self.criterion(outputs, labels)\n\n        # No distillation loss if configured\n        if self.cfg.deit_loss_type == \"none\":\n            return loss_base\n\n        # Teacher predictions (global and dense logits)\n        with torch.no_grad():\n            tea_dense_logits, tea_global_logits = self.teacher(inputs)\n\n        # DeiT-based distillation loss\n        loss_deit = self.get_loss_deit(stu_deit_logits, tea_global_logits)\n\n        # Cumulative spatial knowledge distillation loss\n        loss_cskd = self.get_loss_cskd(\n            stu_dense_logits, tea_dense_logits, tea_global_logits, epoch, max_epoch\n        )\n\n        # Weighted combination of losses\n        alpha = self.cfg.deit_alpha\n        total_loss = (\n            loss_base * (1 - alpha)\n            + loss_deit * alpha\n            + loss_cskd * self.cfg.cksd_loss_weight\n        )\n        return total_loss\n        \n    def align_stu_logits(self, stu_dense_logits):\n        \"\"\"\n        Align student logits to match teacher logits' spatial resolution.\n        \"\"\"\n        N, M, C = stu_dense_logits.shape  # Batch size, number of patches, num_classes\n    \n        # Dynamically compute H and W based on the number of patches\n        H = int(math.sqrt(M)) if math.sqrt(M).is_integer() else int(math.ceil(math.sqrt(M)))\n        W = int(M / H)\n    \n        if H * W != M:\n            raise ValueError(f\"Number of patches {M} cannot be reshaped into a valid grid (H={H}, W={W}).\")\n    \n        # Reshape and align\n        stu_dense_logits = stu_dense_logits.permute(0, 2, 1).reshape(N, C, H, W)\n        stu_dense_logits = F.avg_pool2d(stu_dense_logits, kernel_size=2, stride=2)\n        return stu_dense_logits\n\n\n\n    def get_decay_ratio(self, epoch, max_epoch):\n        \"\"\"\n        Compute the decay ratio for combining dense and global teacher logits.\n        \"\"\"\n        x = epoch / max_epoch\n        if self.cfg.cskd_decay_func == \"linear\":\n            return 1 - x\n        elif self.cfg.cskd_decay_func == \"x2\":\n            return (1 - x) ** 2\n        elif self.cfg.cskd_decay_func == \"cos\":\n            return math.cos(math.pi * 0.5 * x)\n        else:\n            raise NotImplementedError(f\"Decay function '{self.cfg.cskd_decay_func}' not implemented.\")\n\n    def get_loss_deit(self, stu_deit_logits, tea_global_logits):\n        \"\"\"\n        Compute the DeiT distillation loss.\n        \"\"\"\n        if self.cfg.deit_loss_type == \"soft\":\n            T = self.cfg.deit_tau  # Temperature\n            loss_deit = F.kl_div(\n                F.log_softmax(stu_deit_logits / T, dim=1),\n                F.log_softmax(tea_global_logits / T, dim=1),\n                reduction=\"sum\",\n                log_target=True,\n            ) * (T * T) / stu_deit_logits.numel()\n        elif self.cfg.deit_loss_type == \"hard\":\n            loss_deit = F.cross_entropy(\n                stu_deit_logits, tea_global_logits.argmax(dim=1)\n            )\n        else:\n            raise NotImplementedError(f\"DeiT loss type '{self.cfg.deit_loss_type}' not implemented.\")\n        return loss_deit\n\n    def get_loss_cskd(self, stu_dense_logits, tea_dense_logits, tea_global_logits, epoch, max_epoch):\n        \"\"\"\n        Compute the cumulative spatial knowledge distillation loss.\n        \"\"\"\n        # Align student logits to teacher's spatial resolution\n        stu_dense_logits = self.align_stu_logits(stu_dense_logits)\n\n        # Compute the decay ratio\n        decay_ratio = self.get_decay_ratio(epoch, max_epoch)\n\n        # Weighted combination of dense and global teacher logits\n        N, C = tea_global_logits.shape\n        teacher_logits = (\n            decay_ratio * tea_dense_logits\n            + (1 - decay_ratio) * tea_global_logits.reshape(N, C, 1, 1)\n        )\n\n        # CSKD loss\n        if self.cfg.deit_loss_type == \"hard\":\n            loss_cskd = F.cross_entropy(\n                stu_dense_logits, teacher_logits.argmax(dim=1)\n            )\n        elif self.cfg.deit_loss_type == \"soft\":\n            T = self.cfg.deit_tau  # Temperature\n            loss_cskd = F.kl_div(\n                F.log_softmax(stu_dense_logits / T, dim=1),\n                F.log_softmax(teacher_logits / T, dim=1),\n                reduction=\"sum\",\n                log_target=True,\n            ) * (T * T) / stu_dense_logits.size(0)\n        else:\n            raise NotImplementedError(f\"CSKD loss type '{self.cfg.cskd_loss_type}' not implemented.\")\n        return loss_cskd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:35:08.187184Z","iopub.execute_input":"2024-12-25T16:35:08.187523Z","iopub.status.idle":"2024-12-25T16:35:08.199986Z","shell.execute_reply.started":"2024-12-25T16:35:08.187498Z","shell.execute_reply":"2024-12-25T16:35:08.199040Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def train_student_with_cskd(student, teacher, train_loader, cfg):\n    \"\"\"\n    Train a student model using CSKD with configurations from a ConfigBase class.\n\n    Args:\n        student: The student model (e.g., Vision Transformer).\n        teacher: The teacher model (e.g., ResNet).\n        train_loader: DataLoader for training data.\n        cfg: Configuration object from a ConfigBase class instance.\n    \"\"\"\n    student.train()\n    teacher.eval()\n\n    # Initialize optimizer and loss functions from config\n    optimizer = torch.optim.Adam(student.parameters(), lr=cfg.learning_rate)\n    criterion = nn.CrossEntropyLoss()\n    cskd_loss = CSKDLoss(cfg, criterion, teacher)\n\n    for epoch in range(cfg.num_epochs):\n        running_loss = 0.0\n        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.num_epochs}\"):\n            inputs, labels = inputs.to(cfg.device), labels.to(cfg.device)\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = student(inputs)\n\n            # Compute CSKD loss\n            loss = cskd_loss(inputs, outputs, labels, epoch, cfg.num_epochs)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch [{epoch+1}/{cfg.num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:35:16.453642Z","iopub.execute_input":"2024-12-25T16:35:16.453926Z","iopub.status.idle":"2024-12-25T16:35:16.459655Z","shell.execute_reply.started":"2024-12-25T16:35:16.453905Z","shell.execute_reply":"2024-12-25T16:35:16.458914Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from functools import lru_cache\nfrom hashlib import md5\nfrom pprint import pprint\n\nclass ConfigBase:\n    @classmethod\n    @lru_cache(maxsize=1)\n    def to_dict(cls):\n        keys = dir(cls)\n        hp_dict = {}\n        for key in keys:\n            value = getattr(cls, key)\n            if (\n                not key.startswith(\"__\")\n                and not key.endswith(\"__\")\n                and not callable(value)\n            ):\n                hp_dict[key] = value\n        return hp_dict\n\n    @classmethod\n    @lru_cache(maxsize=1)\n    def to_md5(cls):\n        hp_dict = cls.to_dict()\n        return md5(f\"{hp_dict}\".encode(\"utf-8\")).hexdigest()\n\n    @classmethod\n    @lru_cache(maxsize=1)\n    def instance(cls):\n        return cls()\n\n    @classmethod\n    def print(cls):\n        print(f\"hash (md5): {cls.to_md5()}\")\n        pprint(cls.to_dict())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:35:29.675621Z","iopub.execute_input":"2024-12-25T16:35:29.675937Z","iopub.status.idle":"2024-12-25T16:35:29.681785Z","shell.execute_reply.started":"2024-12-25T16:35:29.675905Z","shell.execute_reply":"2024-12-25T16:35:29.681065Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class MyConfig(ConfigBase):\n    # General settings\n    seed = 42\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Dataset settings\n    dataset_path = \"./data\"\n    batch_size = 32\n    num_workers = 4\n\n    # Training settings\n    learning_rate = 0.001\n    num_epochs = 1\n    deit_alpha = 0.5\n    cksd_loss_weight = 1.0\n    deit_loss_type = \"soft\"  # or \"hard\"\n    deit_tau = 2.0\n    cskd_decay_func = \"linear\"  # \"x2\" or \"cos\"\n\n    # Model settings\n    model_name = \"vit_base_patch16_224\"\n    teacher_model_name = \"resnet50\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:35:32.629130Z","iopub.execute_input":"2024-12-25T16:35:32.629443Z","iopub.status.idle":"2024-12-25T16:35:32.634155Z","shell.execute_reply.started":"2024-12-25T16:35:32.629418Z","shell.execute_reply":"2024-12-25T16:35:32.633274Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Load configuration\ncfg = MyConfig.instance()\n\n# Print the configuration\ncfg.print()\n\n# Access configuration parameters\ndevice = cfg.device\nbatch_size = cfg.batch_size\nlearning_rate = cfg.learning_rate\n\nprint(f\"Running on device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:35:37.112790Z","iopub.execute_input":"2024-12-25T16:35:37.113095Z","iopub.status.idle":"2024-12-25T16:35:37.122947Z","shell.execute_reply.started":"2024-12-25T16:35:37.113068Z","shell.execute_reply":"2024-12-25T16:35:37.122090Z"}},"outputs":[{"name":"stdout","text":"hash (md5): 1db02d5f2b9fb6deb889c8c31bba3e3c\n{'batch_size': 32,\n 'cksd_loss_weight': 1.0,\n 'cskd_decay_func': 'linear',\n 'dataset_path': './data',\n 'deit_alpha': 0.5,\n 'deit_loss_type': 'soft',\n 'deit_tau': 2.0,\n 'device': 'cuda',\n 'learning_rate': 0.001,\n 'model_name': 'vit_base_patch16_224',\n 'num_epochs': 1,\n 'num_workers': 4,\n 'seed': 42,\n 'teacher_model_name': 'resnet50'}\nRunning on device: cuda\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import torchvision.models as models\n\nclass ResNetWithDenseOutputs(nn.Module):\n    def __init__(self, base_model, num_classes=10):\n        super().__init__()\n        self.base_model = nn.Sequential(*list(base_model.children())[:-2]) \n          # remove avgpool + fc\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = base_model.fc            # the standard classification fc for global logits\n\n        # NEW: 1×1 conv to map 2048 → num_classes\n        self.dense_head = nn.Conv2d(2048, num_classes, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        # features => [B, 2048, 7, 7]\n        features = self.base_model(x)\n\n        # Global logits => [B, 10]\n        global_logits = self.fc(\n            self.global_pool(features).squeeze(-1).squeeze(-1)\n        )\n\n        # Dense logits => [B, 10, 7, 7]\n        dense_logits = self.dense_head(features)\n\n        return dense_logits, global_logits\n\n        return features, global_logits\ndef create_resnet_model(num_classes=10):\n    base_model = models.resnet50(pretrained=True)\n    base_model.fc = nn.Linear(base_model.fc.in_features, num_classes)  # Update the classification head\n    return ResNetWithDenseOutputs(base_model)\n\ndef get_model(name):\n    if name == 'student':\n        vit_model = create_vit_model()\n        return vit_model\n    elif name == 'teacher':\n        resnet_model = create_resnet_model(num_classes=10)\n        return resnet_model\n    else:\n        raise ValueError(f\"Unsupported model name: {name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:35:44.247466Z","iopub.execute_input":"2024-12-25T16:35:44.247754Z","iopub.status.idle":"2024-12-25T16:35:44.254419Z","shell.execute_reply.started":"2024-12-25T16:35:44.247731Z","shell.execute_reply":"2024-12-25T16:35:44.253609Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def finetune_model(model, train_loader, num_epochs=10, alpha=1e-3):\n    \"\"\"\n    Fine-tune the model on the training data.\n    If model(inputs) returns (dense_logits, global_logits), we only\n    use the global_logits for cross-entropy.\n    \"\"\"\n    print(\"Finetuning the model on this data\")\n    model.train()\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=alpha)\n    num_batches = len(train_loader)\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for batch_index, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            # `model(inputs)` may return a tuple. Let's unpack it:\n            outputs_tuple = model(inputs)\n\n            # If the teacher model returns (dense_logits, global_logits):\n            #   outputs = outputs_tuple[0]  # dense_logits\n            #   global_logits = outputs_tuple[1]\n            # If you want to train on global logits, do:\n            if isinstance(outputs_tuple, tuple):\n                # Suppose the second element is the global logits\n                outputs = outputs_tuple[1]\n            else:\n                # otherwise, if the model returns a single tensor\n                outputs = outputs_tuple\n            \n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            # Calculate accuracy\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            # Print info periodically\n            if batch_index % 300 == 0 or batch_index == 750:\n                print(f'Batch {batch_index + 1}/{num_batches} - '\n                      f'Loss: {loss.item():.4f} Accuracy: {(correct/total) * 100:.2f}%')\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy = correct / total\n        print(f'------------------------->Epoch [{epoch + 1}/{num_epochs}], '\n              f'Loss: {epoch_loss:.4f} Accuracy : {epoch_accuracy * 100:.2f}%')\n\n    return epoch_loss, epoch_accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T10:59:00.752298Z","iopub.execute_input":"2024-12-24T10:59:00.752613Z","iopub.status.idle":"2024-12-24T10:59:00.759530Z","shell.execute_reply.started":"2024-12-24T10:59:00.752591Z","shell.execute_reply":"2024-12-24T10:59:00.758663Z"}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"# Create the teacher model\nteacher_model = get_model('teacher').to(cfg.device)\n\n# 2. Finetune the teacher on CIFAR-10\nteacher_loss, teacher_accuracy = finetune_model(\n    teacher_model, \n    train_loader,     # CIFAR-10 train_loader\n    num_epochs=10, \n    alpha=1e-3\n)\n\nprint(f\"Teacher finetuned on CIFAR-10 with final loss={teacher_loss:.4f}, accuracy={teacher_accuracy*100:.2f}%\")\n\ntorch.save(teacher_model.state_dict(), 'teacher_model_cskd_weights_after_finetuning.pth')\n\n\nprint(\"Teacher model weights saved successfully!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T10:59:03.458365Z","iopub.execute_input":"2024-12-24T10:59:03.458660Z","iopub.status.idle":"2024-12-24T12:30:50.902061Z","shell.execute_reply.started":"2024-12-24T10:59:03.458635Z","shell.execute_reply":"2024-12-24T12:30:50.901142Z"}},"outputs":[{"name":"stdout","text":"Finetuning the model on this data\nBatch 1/1563 - Loss: 2.2401 Accuracy: 15.62%\nBatch 301/1563 - Loss: 1.7760 Accuracy: 49.87%\nBatch 601/1563 - Loss: 0.8437 Accuracy: 57.11%\nBatch 751/1563 - Loss: 0.7100 Accuracy: 59.60%\nBatch 901/1563 - Loss: 0.8096 Accuracy: 61.55%\nBatch 1201/1563 - Loss: 0.5885 Accuracy: 64.45%\nBatch 1501/1563 - Loss: 0.6351 Accuracy: 66.52%\n------------------------->Epoch [1/10], Loss: 0.9409 Accuracy : 66.92%\nBatch 1/1563 - Loss: 0.6178 Accuracy: 84.38%\nBatch 301/1563 - Loss: 0.5414 Accuracy: 78.85%\nBatch 601/1563 - Loss: 0.4257 Accuracy: 79.61%\nBatch 751/1563 - Loss: 0.3871 Accuracy: 79.56%\nBatch 901/1563 - Loss: 0.5903 Accuracy: 79.82%\nBatch 1201/1563 - Loss: 0.4419 Accuracy: 80.41%\nBatch 1501/1563 - Loss: 0.4739 Accuracy: 80.86%\n------------------------->Epoch [2/10], Loss: 0.5563 Accuracy : 80.94%\nBatch 1/1563 - Loss: 0.3159 Accuracy: 90.62%\nBatch 301/1563 - Loss: 0.5521 Accuracy: 84.74%\nBatch 601/1563 - Loss: 0.4509 Accuracy: 84.93%\nBatch 751/1563 - Loss: 0.3722 Accuracy: 85.02%\nBatch 901/1563 - Loss: 0.2460 Accuracy: 84.94%\nBatch 1201/1563 - Loss: 0.5692 Accuracy: 85.18%\nBatch 1501/1563 - Loss: 0.2176 Accuracy: 85.42%\n------------------------->Epoch [3/10], Loss: 0.4219 Accuracy : 85.45%\nBatch 1/1563 - Loss: 0.6974 Accuracy: 81.25%\nBatch 301/1563 - Loss: 0.3694 Accuracy: 88.77%\nBatch 601/1563 - Loss: 0.5192 Accuracy: 88.48%\nBatch 751/1563 - Loss: 0.4934 Accuracy: 88.22%\nBatch 901/1563 - Loss: 0.4701 Accuracy: 88.35%\nBatch 1201/1563 - Loss: 0.2433 Accuracy: 88.30%\nBatch 1501/1563 - Loss: 0.0741 Accuracy: 88.30%\n------------------------->Epoch [4/10], Loss: 0.3350 Accuracy : 88.30%\nBatch 1/1563 - Loss: 0.2178 Accuracy: 90.62%\nBatch 301/1563 - Loss: 0.0797 Accuracy: 91.19%\nBatch 601/1563 - Loss: 0.4474 Accuracy: 90.84%\nBatch 751/1563 - Loss: 0.2360 Accuracy: 90.79%\nBatch 901/1563 - Loss: 0.1843 Accuracy: 90.79%\nBatch 1201/1563 - Loss: 0.2235 Accuracy: 90.67%\nBatch 1501/1563 - Loss: 0.2883 Accuracy: 90.57%\n------------------------->Epoch [5/10], Loss: 0.2707 Accuracy : 90.60%\nBatch 1/1563 - Loss: 0.4315 Accuracy: 84.38%\nBatch 301/1563 - Loss: 0.1563 Accuracy: 93.42%\nBatch 601/1563 - Loss: 0.3866 Accuracy: 92.91%\nBatch 751/1563 - Loss: 0.1518 Accuracy: 92.57%\nBatch 901/1563 - Loss: 0.2148 Accuracy: 92.36%\nBatch 1201/1563 - Loss: 0.2765 Accuracy: 92.28%\nBatch 1501/1563 - Loss: 0.1139 Accuracy: 92.27%\n------------------------->Epoch [6/10], Loss: 0.2196 Accuracy : 92.27%\nBatch 1/1563 - Loss: 0.2483 Accuracy: 90.62%\nBatch 301/1563 - Loss: 0.2310 Accuracy: 94.95%\nBatch 601/1563 - Loss: 0.1471 Accuracy: 94.85%\nBatch 751/1563 - Loss: 0.2671 Accuracy: 94.74%\nBatch 901/1563 - Loss: 0.1893 Accuracy: 94.49%\nBatch 1201/1563 - Loss: 0.1549 Accuracy: 94.22%\nBatch 1501/1563 - Loss: 0.2261 Accuracy: 94.09%\n------------------------->Epoch [7/10], Loss: 0.1718 Accuracy : 94.06%\nBatch 1/1563 - Loss: 0.2133 Accuracy: 93.75%\nBatch 301/1563 - Loss: 0.0258 Accuracy: 96.13%\nBatch 601/1563 - Loss: 0.1460 Accuracy: 95.63%\nBatch 751/1563 - Loss: 0.2230 Accuracy: 95.52%\nBatch 901/1563 - Loss: 0.1514 Accuracy: 95.44%\nBatch 1201/1563 - Loss: 0.0215 Accuracy: 95.12%\nBatch 1501/1563 - Loss: 0.2094 Accuracy: 95.08%\n------------------------->Epoch [8/10], Loss: 0.1374 Accuracy : 95.10%\nBatch 1/1563 - Loss: 0.0270 Accuracy: 100.00%\nBatch 301/1563 - Loss: 0.1724 Accuracy: 96.91%\nBatch 601/1563 - Loss: 0.2298 Accuracy: 96.35%\nBatch 751/1563 - Loss: 0.3031 Accuracy: 96.07%\nBatch 901/1563 - Loss: 0.0222 Accuracy: 96.07%\nBatch 1201/1563 - Loss: 0.0294 Accuracy: 96.04%\nBatch 1501/1563 - Loss: 0.0966 Accuracy: 95.94%\n------------------------->Epoch [9/10], Loss: 0.1146 Accuracy : 95.91%\nBatch 1/1563 - Loss: 0.0390 Accuracy: 100.00%\nBatch 301/1563 - Loss: 0.0364 Accuracy: 97.31%\nBatch 601/1563 - Loss: 0.0034 Accuracy: 97.32%\nBatch 751/1563 - Loss: 0.1071 Accuracy: 97.30%\nBatch 901/1563 - Loss: 0.0835 Accuracy: 97.07%\nBatch 1201/1563 - Loss: 0.0864 Accuracy: 96.63%\nBatch 1501/1563 - Loss: 0.0791 Accuracy: 96.60%\n------------------------->Epoch [10/10], Loss: 0.0965 Accuracy : 96.62%\nTeacher finetuned on CIFAR-10 with final loss=0.0965, accuracy=96.62%\nTeacher model weights saved successfully!\n","output_type":"stream"}],"execution_count":137},{"cell_type":"code","source":"from torch.utils.data import Subset\nimport torch\n\n# Get the original dataset from the DataLoader\ntrain_dataset = train_loader.dataset\n\n# Create a subset of the dataset with only the first 1000 images\nsubset_indices = torch.arange(5000)\ntrain_subset = Subset(train_dataset, subset_indices)\n\n# Create a DataLoader for the subset\nsubset_loader = torch.utils.data.DataLoader(train_subset, batch_size=cfg.batch_size, shuffle=True)\n\n# Train the student model using the subset DataLoader\nstudent_model_test = get_model('student')\nstudent_model_test = student_model_test.to(device)\n\ntrain_student_with_cskd(student_model_test, teacher_model, subset_loader, cfg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T13:39:18.031908Z","iopub.execute_input":"2024-12-24T13:39:18.032269Z","iopub.status.idle":"2024-12-24T13:40:29.932024Z","shell.execute_reply.started":"2024-12-24T13:39:18.032241Z","shell.execute_reply":"2024-12-24T13:40:29.931231Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 157/157 [01:10<00:00,  2.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 6.0566\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":152},{"cell_type":"code","source":"# Initialize models using config\n# student_model_cskd = create_model(cfg.model_name, pretrained=True, num_classes=10).to(cfg.device)\n\nstudent_model_cskd = get_model('student')\nstudent_model_cskd = student_model_cskd.to(device)\n\ntrain_student_with_cskd(student_model_cskd, teacher_model, train_loader, cfg)\n\n# Define paths to save the model weights and features\nweights_save_path = \"student_model_cskd_weights.pth\"\nfeatures_save_path = \"student_model_cskd_features.pth\"\n\n# Save the model weights\ntorch.save(student_model_cskd.state_dict(), weights_save_path)\nprint(f\"Model weights saved to {weights_save_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T13:40:29.932922Z","iopub.execute_input":"2024-12-24T13:40:29.933118Z","iopub.status.idle":"2024-12-24T13:53:06.357244Z","shell.execute_reply.started":"2024-12-24T13:40:29.933101Z","shell.execute_reply":"2024-12-24T13:53:06.356484Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 1563/1563 [12:34<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.7197\nModel weights saved to student_model_cskd_weights.pth\n","output_type":"stream"}],"execution_count":153},{"cell_type":"code","source":"def evaluate_cskd_model(model, data_loader, device):\n    \"\"\"Evaluate the models using a basic testing loop\"\"\"\n    print(\"Evaluating the model\")\n    model.to(device)\n    model.eval()\n    correct = 0\n    total = 0\n    num_batches = len(data_loader)  # Get total number of batches\n\n    with torch.no_grad():\n        for batch_index, (images, labels) in enumerate(data_loader):\n\n            batch_size = images.size(0)\n            if batch_size < 32:\n                print(f\"Skipping last batch of size {batch_size} --- gives shape error\")\n                break\n\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n\n            # Handle tuple outputs\n            if isinstance(outputs, tuple):\n                outputs = outputs[0]  # Extract global_logits\n\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            # Print progress every 10 batches\n            if batch_index % 10 == 0:\n                print(f'Batch {batch_index + 1}/{num_batches} - Accuracy: { (correct/total) * 100:.2f}%')\n\n    accuracy = correct / total\n    print(f'Final Accuracy: {accuracy * 100:.2f}%')\n    return accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:38:24.268183Z","iopub.execute_input":"2024-12-25T16:38:24.268504Z","iopub.status.idle":"2024-12-25T16:38:24.275035Z","shell.execute_reply.started":"2024-12-25T16:38:24.268478Z","shell.execute_reply":"2024-12-25T16:38:24.273977Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"teacher_model = get_model('teacher').to(cfg.device)\n\nstudent_model_cskd = get_model('student')\nstudent_model_cskd = student_model_cskd.to(device)\n\nstudent_model_cskd.load_state_dict(torch.load('/kaggle/input/cskd-distillation/student_model_cskd_weights.pth'))\nteacher_model.load_state_dict(torch.load('/kaggle/input/cskd-distillation/teacher_model_cskd_weights_after_finetuning.pth'))\n\n\nprint(\"Student model weights and features loaded successfully!\")\n\n\n\ntest_cskd_acc = evaluate_cskd_model(student_model_cskd, test_loader, device)\nprint(f\"Student (CSKD) accuracy on test data = {test_cskd_acc*100:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:42:02.949065Z","iopub.execute_input":"2024-12-25T16:42:02.949446Z","iopub.status.idle":"2024-12-25T16:44:22.114513Z","shell.execute_reply.started":"2024-12-25T16:42:02.949413Z","shell.execute_reply":"2024-12-25T16:44:22.113434Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-33-5e9d798ccff2>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student_model_cskd.load_state_dict(torch.load('/kaggle/input/cskd-distillation/student_model_cskd_weights.pth'))\n<ipython-input-33-5e9d798ccff2>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  teacher_model.load_state_dict(torch.load('/kaggle/input/cskd-distillation/teacher_model_cskd_weights_after_finetuning.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Student model weights and features loaded successfully!\nEvaluating the model\nBatch 1/313 - Accuracy: 100.00%\nBatch 11/313 - Accuracy: 94.89%\nBatch 21/313 - Accuracy: 95.09%\nBatch 31/313 - Accuracy: 95.46%\nBatch 41/313 - Accuracy: 95.43%\nBatch 51/313 - Accuracy: 95.22%\nBatch 61/313 - Accuracy: 95.34%\nBatch 71/313 - Accuracy: 95.33%\nBatch 81/313 - Accuracy: 95.25%\nBatch 91/313 - Accuracy: 95.05%\nBatch 101/313 - Accuracy: 94.93%\nBatch 111/313 - Accuracy: 94.88%\nBatch 121/313 - Accuracy: 94.89%\nBatch 131/313 - Accuracy: 94.99%\nBatch 141/313 - Accuracy: 95.17%\nBatch 151/313 - Accuracy: 94.99%\nBatch 161/313 - Accuracy: 94.95%\nBatch 171/313 - Accuracy: 94.99%\nBatch 181/313 - Accuracy: 95.04%\nBatch 191/313 - Accuracy: 95.08%\nBatch 201/313 - Accuracy: 95.12%\nBatch 211/313 - Accuracy: 95.10%\nBatch 221/313 - Accuracy: 95.19%\nBatch 231/313 - Accuracy: 95.25%\nBatch 241/313 - Accuracy: 95.20%\nBatch 251/313 - Accuracy: 95.17%\nBatch 261/313 - Accuracy: 95.15%\nBatch 271/313 - Accuracy: 95.18%\nBatch 281/313 - Accuracy: 95.17%\nBatch 291/313 - Accuracy: 95.12%\nBatch 301/313 - Accuracy: 95.17%\nBatch 311/313 - Accuracy: 95.20%\nSkipping last batch of size 16 --- gives shape error\nFinal Accuracy: 95.20%\nStudent (CSKD) accuracy on test data = 95.20%\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Evaluate on texture-bias dataset\ntexture_cskd_acc = evaluate_cskd_model(student_model_cskd, texture_bias_loader, device)\nprint(f\"Student (CSKD) accuracy on Texture-Bias = {texture_cskd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T14:07:37.998775Z","iopub.execute_input":"2024-12-24T14:07:37.999054Z","iopub.status.idle":"2024-12-24T14:09:18.845315Z","shell.execute_reply.started":"2024-12-24T14:07:37.999033Z","shell.execute_reply":"2024-12-24T14:09:18.844393Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 34.38%\nBatch 11/313 - Accuracy: 36.93%\nBatch 21/313 - Accuracy: 39.14%\nBatch 31/313 - Accuracy: 40.12%\nBatch 41/313 - Accuracy: 40.32%\nBatch 51/313 - Accuracy: 41.24%\nBatch 61/313 - Accuracy: 41.29%\nBatch 71/313 - Accuracy: 41.55%\nBatch 81/313 - Accuracy: 40.93%\nBatch 91/313 - Accuracy: 40.76%\nBatch 101/313 - Accuracy: 40.32%\nBatch 111/313 - Accuracy: 40.40%\nBatch 121/313 - Accuracy: 40.70%\nBatch 131/313 - Accuracy: 40.65%\nBatch 141/313 - Accuracy: 40.89%\nBatch 151/313 - Accuracy: 40.79%\nBatch 161/313 - Accuracy: 40.97%\nBatch 171/313 - Accuracy: 40.95%\nBatch 181/313 - Accuracy: 40.71%\nBatch 191/313 - Accuracy: 40.76%\nBatch 201/313 - Accuracy: 40.97%\nBatch 211/313 - Accuracy: 40.89%\nBatch 221/313 - Accuracy: 40.71%\nBatch 231/313 - Accuracy: 40.91%\nBatch 241/313 - Accuracy: 40.68%\nBatch 251/313 - Accuracy: 40.74%\nBatch 261/313 - Accuracy: 40.76%\nBatch 271/313 - Accuracy: 40.56%\nBatch 281/313 - Accuracy: 40.54%\nBatch 291/313 - Accuracy: 40.52%\nBatch 301/313 - Accuracy: 40.50%\nBatch 311/313 - Accuracy: 40.37%\nSkipping last batch of size 16 --- gives shape error\nFinal Accuracy: 40.37%\nStudent (CSKD) accuracy on Texture-Bias = 40.37%\n","output_type":"stream"}],"execution_count":161},{"cell_type":"code","source":"# Evaluate on shape-bias dataset\nshape_cskd_acc = evaluate_cskd_model(student_model_cskd, shape_bias_loader, device)\nprint(f\"Student (CSKD) accuracy on Shape-Bias = {shape_cskd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T14:09:18.846641Z","iopub.execute_input":"2024-12-24T14:09:18.846920Z","iopub.status.idle":"2024-12-24T14:11:41.465513Z","shell.execute_reply.started":"2024-12-24T14:09:18.846898Z","shell.execute_reply":"2024-12-24T14:11:41.464786Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 15.62%\nBatch 11/313 - Accuracy: 16.19%\nBatch 21/313 - Accuracy: 17.71%\nBatch 31/313 - Accuracy: 19.25%\nBatch 41/313 - Accuracy: 18.06%\nBatch 51/313 - Accuracy: 17.22%\nBatch 61/313 - Accuracy: 17.16%\nBatch 71/313 - Accuracy: 17.25%\nBatch 81/313 - Accuracy: 17.01%\nBatch 91/313 - Accuracy: 16.79%\nBatch 101/313 - Accuracy: 16.74%\nBatch 111/313 - Accuracy: 16.55%\nBatch 121/313 - Accuracy: 16.24%\nBatch 131/313 - Accuracy: 16.70%\nBatch 141/313 - Accuracy: 16.71%\nBatch 151/313 - Accuracy: 16.72%\nBatch 161/313 - Accuracy: 16.71%\nBatch 171/313 - Accuracy: 16.89%\nBatch 181/313 - Accuracy: 16.95%\nBatch 191/313 - Accuracy: 17.03%\nBatch 201/313 - Accuracy: 17.06%\nBatch 211/313 - Accuracy: 17.15%\nBatch 221/313 - Accuracy: 16.97%\nBatch 231/313 - Accuracy: 17.00%\nBatch 241/313 - Accuracy: 17.01%\nBatch 251/313 - Accuracy: 17.11%\nBatch 261/313 - Accuracy: 16.97%\nBatch 271/313 - Accuracy: 16.85%\nBatch 281/313 - Accuracy: 16.90%\nBatch 291/313 - Accuracy: 16.92%\nBatch 301/313 - Accuracy: 16.92%\nBatch 311/313 - Accuracy: 16.92%\nSkipping last batch of size 16 --- gives shape error\nFinal Accuracy: 16.94%\nStudent (CSKD) accuracy on Shape-Bias = 16.94%\n","output_type":"stream"}],"execution_count":162},{"cell_type":"code","source":"# Evaluate on scrambled dataset\nscrambled_cskd_acc = evaluate_cskd_model(student_model_cskd, scrambled_loader, device)\nprint(f\"Student (CSKD) accuracy on Scrambled = {scrambled_cskd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T14:11:41.466733Z","iopub.execute_input":"2024-12-24T14:11:41.466988Z","iopub.status.idle":"2024-12-24T14:14:00.659987Z","shell.execute_reply.started":"2024-12-24T14:11:41.466967Z","shell.execute_reply":"2024-12-24T14:14:00.659208Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-16-bf889dc6a427>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 31.25%\nBatch 11/313 - Accuracy: 24.15%\nBatch 21/313 - Accuracy: 22.47%\nBatch 31/313 - Accuracy: 22.08%\nBatch 41/313 - Accuracy: 22.18%\nBatch 51/313 - Accuracy: 21.38%\nBatch 61/313 - Accuracy: 20.54%\nBatch 71/313 - Accuracy: 20.47%\nBatch 81/313 - Accuracy: 20.49%\nBatch 91/313 - Accuracy: 20.36%\nBatch 101/313 - Accuracy: 20.58%\nBatch 111/313 - Accuracy: 20.47%\nBatch 121/313 - Accuracy: 20.14%\nBatch 131/313 - Accuracy: 20.16%\nBatch 141/313 - Accuracy: 20.48%\nBatch 151/313 - Accuracy: 20.84%\nBatch 161/313 - Accuracy: 20.87%\nBatch 171/313 - Accuracy: 21.13%\nBatch 181/313 - Accuracy: 21.22%\nBatch 191/313 - Accuracy: 21.42%\nBatch 201/313 - Accuracy: 21.47%\nBatch 211/313 - Accuracy: 21.53%\nBatch 221/313 - Accuracy: 21.56%\nBatch 231/313 - Accuracy: 21.48%\nBatch 241/313 - Accuracy: 21.50%\nBatch 251/313 - Accuracy: 21.51%\nBatch 261/313 - Accuracy: 21.55%\nBatch 271/313 - Accuracy: 21.63%\nBatch 281/313 - Accuracy: 21.70%\nBatch 291/313 - Accuracy: 21.74%\nBatch 301/313 - Accuracy: 21.76%\nBatch 311/313 - Accuracy: 21.71%\nSkipping last batch of size 16 --- gives shape error\nFinal Accuracy: 21.70%\nStudent (CSKD) accuracy on Scrambled = 21.70%\n","output_type":"stream"}],"execution_count":163},{"cell_type":"code","source":"# Evaluate on noisy dataset\nnoisy_cskd_acc = evaluate_cskd_model(student_model_cskd, noisy_loader, device)\nprint(f\"Student (CSKD) accuracy on Noisy = {noisy_cskd_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T14:14:00.660962Z","iopub.execute_input":"2024-12-24T14:14:00.661242Z","iopub.status.idle":"2024-12-24T14:16:24.707305Z","shell.execute_reply.started":"2024-12-24T14:14:00.661214Z","shell.execute_reply":"2024-12-24T14:16:24.706452Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-17-f30d01fd9078>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 68.75%\nBatch 11/313 - Accuracy: 69.60%\nBatch 21/313 - Accuracy: 71.28%\nBatch 31/313 - Accuracy: 70.77%\nBatch 41/313 - Accuracy: 70.81%\nBatch 51/313 - Accuracy: 71.08%\nBatch 61/313 - Accuracy: 70.54%\nBatch 71/313 - Accuracy: 69.63%\nBatch 81/313 - Accuracy: 69.48%\nBatch 91/313 - Accuracy: 69.78%\nBatch 101/313 - Accuracy: 70.39%\nBatch 111/313 - Accuracy: 70.64%\nBatch 121/313 - Accuracy: 70.74%\nBatch 131/313 - Accuracy: 70.30%\nBatch 141/313 - Accuracy: 70.70%\nBatch 151/313 - Accuracy: 70.51%\nBatch 161/313 - Accuracy: 70.42%\nBatch 171/313 - Accuracy: 70.43%\nBatch 181/313 - Accuracy: 70.46%\nBatch 191/313 - Accuracy: 70.44%\nBatch 201/313 - Accuracy: 70.40%\nBatch 211/313 - Accuracy: 70.26%\nBatch 221/313 - Accuracy: 70.12%\nBatch 231/313 - Accuracy: 70.17%\nBatch 241/313 - Accuracy: 70.10%\nBatch 251/313 - Accuracy: 70.23%\nBatch 261/313 - Accuracy: 70.08%\nBatch 271/313 - Accuracy: 70.01%\nBatch 281/313 - Accuracy: 70.04%\nBatch 291/313 - Accuracy: 69.88%\nBatch 301/313 - Accuracy: 69.90%\nBatch 311/313 - Accuracy: 69.85%\nSkipping last batch of size 16 --- gives shape error\nFinal Accuracy: 69.88%\nStudent (CSKD) accuracy on Noisy = 69.88%\n","output_type":"stream"}],"execution_count":164},{"cell_type":"code","source":"# Evaluate on super-pixelated dataset\nsuper_pixelated_cskd_acc = evaluate_cskd_model(student_model_cskd, superpixel_loader, device)\nprint(f\"Student (CSKD) accuracy on Super Pixelated = {super_pixelated_cskd_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T14:16:24.708138Z","iopub.execute_input":"2024-12-24T14:16:24.708468Z","iopub.status.idle":"2024-12-24T14:19:00.998622Z","shell.execute_reply.started":"2024-12-24T14:16:24.708438Z","shell.execute_reply":"2024-12-24T14:19:00.997805Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 46.88%\nBatch 11/313 - Accuracy: 38.35%\nBatch 21/313 - Accuracy: 36.90%\nBatch 31/313 - Accuracy: 36.90%\nBatch 41/313 - Accuracy: 37.20%\nBatch 51/313 - Accuracy: 37.56%\nBatch 61/313 - Accuracy: 37.19%\nBatch 71/313 - Accuracy: 37.06%\nBatch 81/313 - Accuracy: 37.15%\nBatch 91/313 - Accuracy: 37.71%\nBatch 101/313 - Accuracy: 37.84%\nBatch 111/313 - Accuracy: 38.37%\nBatch 121/313 - Accuracy: 38.09%\nBatch 131/313 - Accuracy: 38.22%\nBatch 141/313 - Accuracy: 38.14%\nBatch 151/313 - Accuracy: 38.43%\nBatch 161/313 - Accuracy: 38.47%\nBatch 171/313 - Accuracy: 38.36%\nBatch 181/313 - Accuracy: 38.36%\nBatch 191/313 - Accuracy: 38.35%\nBatch 201/313 - Accuracy: 38.46%\nBatch 211/313 - Accuracy: 38.33%\nBatch 221/313 - Accuracy: 38.24%\nBatch 231/313 - Accuracy: 38.34%\nBatch 241/313 - Accuracy: 38.28%\nBatch 251/313 - Accuracy: 38.23%\nBatch 261/313 - Accuracy: 37.94%\nBatch 271/313 - Accuracy: 37.65%\nBatch 281/313 - Accuracy: 37.88%\nBatch 291/313 - Accuracy: 37.85%\nBatch 301/313 - Accuracy: 37.78%\nBatch 311/313 - Accuracy: 37.81%\nSkipping last batch of size 16 --- gives shape error\nFinal Accuracy: 37.85%\nStudent (CSKD) accuracy on Super Pixelated = 37.85%\n","output_type":"stream"}],"execution_count":165},{"cell_type":"markdown","source":"# Evaluating Resnet (Teaher)","metadata":{}},{"cell_type":"code","source":"test_teacher_acc = evaluate_model(teacher_model, test_loader, device)\nprint(f\"Teacher accuracy on Texture-Bias dataset = {test_teacher_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:33:50.137777Z","iopub.execute_input":"2024-12-25T17:33:50.138152Z","iopub.status.idle":"2024-12-25T17:34:29.928295Z","shell.execute_reply.started":"2024-12-25T17:33:50.138122Z","shell.execute_reply":"2024-12-25T17:34:29.927323Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 87.50%\nBatch 11/313 - Accuracy: 80.97%\nBatch 21/313 - Accuracy: 81.25%\nBatch 31/313 - Accuracy: 81.25%\nBatch 41/313 - Accuracy: 81.25%\nBatch 51/313 - Accuracy: 81.25%\nBatch 61/313 - Accuracy: 81.25%\nBatch 71/313 - Accuracy: 81.16%\nBatch 81/313 - Accuracy: 80.83%\nBatch 91/313 - Accuracy: 80.63%\nBatch 101/313 - Accuracy: 80.63%\nBatch 111/313 - Accuracy: 80.43%\nBatch 121/313 - Accuracy: 80.60%\nBatch 131/313 - Accuracy: 80.77%\nBatch 141/313 - Accuracy: 81.07%\nBatch 151/313 - Accuracy: 81.08%\nBatch 161/313 - Accuracy: 81.23%\nBatch 171/313 - Accuracy: 81.29%\nBatch 181/313 - Accuracy: 81.35%\nBatch 191/313 - Accuracy: 81.36%\nBatch 201/313 - Accuracy: 81.55%\nBatch 211/313 - Accuracy: 81.61%\nBatch 221/313 - Accuracy: 81.50%\nBatch 231/313 - Accuracy: 81.70%\nBatch 241/313 - Accuracy: 81.77%\nBatch 251/313 - Accuracy: 81.80%\nBatch 261/313 - Accuracy: 81.76%\nBatch 271/313 - Accuracy: 81.76%\nBatch 281/313 - Accuracy: 81.76%\nBatch 291/313 - Accuracy: 81.83%\nBatch 301/313 - Accuracy: 81.92%\nBatch 311/313 - Accuracy: 81.93%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 81.90%\nTeacher accuracy on Texture-Bias dataset = 81.90%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Evaluate on shape-bias dataset\nshape_teacher_acc = evaluate_model(teacher_model, shape_bias_loader, device)\nprint(f\"Student (CSKD) accuracy on Shape-Bias = {shape_teacher_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:34:29.929557Z","iopub.execute_input":"2024-12-25T17:34:29.929835Z","iopub.status.idle":"2024-12-25T17:36:43.425620Z","shell.execute_reply.started":"2024-12-25T17:34:29.929811Z","shell.execute_reply":"2024-12-25T17:36:43.424776Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 12.50%\nBatch 11/313 - Accuracy: 10.80%\nBatch 21/313 - Accuracy: 10.42%\nBatch 31/313 - Accuracy: 9.88%\nBatch 41/313 - Accuracy: 9.68%\nBatch 51/313 - Accuracy: 9.19%\nBatch 61/313 - Accuracy: 9.58%\nBatch 71/313 - Accuracy: 9.64%\nBatch 81/313 - Accuracy: 9.68%\nBatch 91/313 - Accuracy: 9.86%\nBatch 101/313 - Accuracy: 9.81%\nBatch 111/313 - Accuracy: 9.71%\nBatch 121/313 - Accuracy: 9.53%\nBatch 131/313 - Accuracy: 9.64%\nBatch 141/313 - Accuracy: 9.73%\nBatch 151/313 - Accuracy: 9.60%\nBatch 161/313 - Accuracy: 9.55%\nBatch 171/313 - Accuracy: 9.61%\nBatch 181/313 - Accuracy: 9.62%\nBatch 191/313 - Accuracy: 9.57%\nBatch 201/313 - Accuracy: 9.64%\nBatch 211/313 - Accuracy: 9.75%\nBatch 221/313 - Accuracy: 9.79%\nBatch 231/313 - Accuracy: 9.78%\nBatch 241/313 - Accuracy: 9.79%\nBatch 251/313 - Accuracy: 9.85%\nBatch 261/313 - Accuracy: 9.69%\nBatch 271/313 - Accuracy: 9.63%\nBatch 281/313 - Accuracy: 9.63%\nBatch 291/313 - Accuracy: 9.59%\nBatch 301/313 - Accuracy: 9.55%\nBatch 311/313 - Accuracy: 9.59%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 9.62%\nStudent (CSKD) accuracy on Shape-Bias = 9.62%\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Evaluate on scrambled dataset\nscrambled_teacher_acc = evaluate_model(teacher_model, scrambled_loader, device)\nprint(f\"Student (CSKD) accuracy on Scrambled = {scrambled_teacher_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:36:43.427185Z","iopub.execute_input":"2024-12-25T17:36:43.427426Z","iopub.status.idle":"2024-12-25T17:38:48.998899Z","shell.execute_reply.started":"2024-12-25T17:36:43.427405Z","shell.execute_reply":"2024-12-25T17:38:48.998080Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-9-bf889dc6a427>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 21.88%\nBatch 11/313 - Accuracy: 24.72%\nBatch 21/313 - Accuracy: 24.85%\nBatch 31/313 - Accuracy: 24.90%\nBatch 41/313 - Accuracy: 24.54%\nBatch 51/313 - Accuracy: 23.96%\nBatch 61/313 - Accuracy: 24.44%\nBatch 71/313 - Accuracy: 24.21%\nBatch 81/313 - Accuracy: 24.50%\nBatch 91/313 - Accuracy: 24.38%\nBatch 101/313 - Accuracy: 24.35%\nBatch 111/313 - Accuracy: 24.52%\nBatch 121/313 - Accuracy: 24.46%\nBatch 131/313 - Accuracy: 24.28%\nBatch 141/313 - Accuracy: 24.18%\nBatch 151/313 - Accuracy: 24.23%\nBatch 161/313 - Accuracy: 24.20%\nBatch 171/313 - Accuracy: 24.10%\nBatch 181/313 - Accuracy: 24.12%\nBatch 191/313 - Accuracy: 24.20%\nBatch 201/313 - Accuracy: 24.28%\nBatch 211/313 - Accuracy: 24.32%\nBatch 221/313 - Accuracy: 24.28%\nBatch 231/313 - Accuracy: 24.23%\nBatch 241/313 - Accuracy: 24.44%\nBatch 251/313 - Accuracy: 24.38%\nBatch 261/313 - Accuracy: 24.19%\nBatch 271/313 - Accuracy: 24.04%\nBatch 281/313 - Accuracy: 24.07%\nBatch 291/313 - Accuracy: 24.12%\nBatch 301/313 - Accuracy: 24.16%\nBatch 311/313 - Accuracy: 24.05%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 24.06%\nStudent (CSKD) accuracy on Scrambled = 24.06%\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Evaluate on noisy dataset\nnoisy_teacher_acc = evaluate_model(teacher_model, noisy_loader, device)\nprint(f\"Student (CSKD) accuracy on Noisy = {noisy_teacher_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:38:48.999938Z","iopub.execute_input":"2024-12-25T17:38:49.000248Z","iopub.status.idle":"2024-12-25T17:41:04.339007Z","shell.execute_reply.started":"2024-12-25T17:38:49.000217Z","shell.execute_reply":"2024-12-25T17:41:04.338225Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-10-f30d01fd9078>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  image = torch.load(img_path)  # Load the .pt tensor file\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/313 - Accuracy: 18.75%\nBatch 11/313 - Accuracy: 16.48%\nBatch 21/313 - Accuracy: 15.62%\nBatch 31/313 - Accuracy: 15.42%\nBatch 41/313 - Accuracy: 15.85%\nBatch 51/313 - Accuracy: 15.26%\nBatch 61/313 - Accuracy: 15.52%\nBatch 71/313 - Accuracy: 16.15%\nBatch 81/313 - Accuracy: 16.24%\nBatch 91/313 - Accuracy: 16.24%\nBatch 101/313 - Accuracy: 16.31%\nBatch 111/313 - Accuracy: 16.27%\nBatch 121/313 - Accuracy: 16.14%\nBatch 131/313 - Accuracy: 16.03%\nBatch 141/313 - Accuracy: 16.16%\nBatch 151/313 - Accuracy: 15.98%\nBatch 161/313 - Accuracy: 15.74%\nBatch 171/313 - Accuracy: 15.83%\nBatch 181/313 - Accuracy: 15.68%\nBatch 191/313 - Accuracy: 15.61%\nBatch 201/313 - Accuracy: 15.72%\nBatch 211/313 - Accuracy: 15.74%\nBatch 221/313 - Accuracy: 15.79%\nBatch 231/313 - Accuracy: 15.83%\nBatch 241/313 - Accuracy: 15.77%\nBatch 251/313 - Accuracy: 15.95%\nBatch 261/313 - Accuracy: 15.78%\nBatch 271/313 - Accuracy: 15.72%\nBatch 281/313 - Accuracy: 15.69%\nBatch 291/313 - Accuracy: 15.61%\nBatch 301/313 - Accuracy: 15.57%\nBatch 311/313 - Accuracy: 15.62%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 15.69%\nStudent (CSKD) accuracy on Noisy = 15.69%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Evaluate on super-pixelated dataset\nsuperpixelated_teacher_acc = evaluate_model(teacher_model, superpixel_loader, device)\nprint(f\"Student (CSKD) accuracy on Super Pixelated = {superpixelated_teacher_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T17:41:04.339950Z","iopub.execute_input":"2024-12-25T17:41:04.340276Z","iopub.status.idle":"2024-12-25T17:43:44.365795Z","shell.execute_reply.started":"2024-12-25T17:41:04.340243Z","shell.execute_reply":"2024-12-25T17:43:44.364847Z"}},"outputs":[{"name":"stdout","text":"Evaluating the model\nBatch 1/313 - Accuracy: 12.50%\nBatch 11/313 - Accuracy: 15.06%\nBatch 21/313 - Accuracy: 13.99%\nBatch 31/313 - Accuracy: 13.61%\nBatch 41/313 - Accuracy: 13.11%\nBatch 51/313 - Accuracy: 12.56%\nBatch 61/313 - Accuracy: 12.91%\nBatch 71/313 - Accuracy: 12.90%\nBatch 81/313 - Accuracy: 12.89%\nBatch 91/313 - Accuracy: 13.19%\nBatch 101/313 - Accuracy: 13.15%\nBatch 111/313 - Accuracy: 13.29%\nBatch 121/313 - Accuracy: 13.07%\nBatch 131/313 - Accuracy: 13.00%\nBatch 141/313 - Accuracy: 12.94%\nBatch 151/313 - Accuracy: 12.83%\nBatch 161/313 - Accuracy: 12.75%\nBatch 171/313 - Accuracy: 12.88%\nBatch 181/313 - Accuracy: 12.86%\nBatch 191/313 - Accuracy: 12.89%\nBatch 201/313 - Accuracy: 12.87%\nBatch 211/313 - Accuracy: 12.99%\nBatch 221/313 - Accuracy: 13.02%\nBatch 231/313 - Accuracy: 12.96%\nBatch 241/313 - Accuracy: 12.94%\nBatch 251/313 - Accuracy: 13.02%\nBatch 261/313 - Accuracy: 12.85%\nBatch 271/313 - Accuracy: 12.78%\nBatch 281/313 - Accuracy: 12.89%\nBatch 291/313 - Accuracy: 12.89%\nBatch 301/313 - Accuracy: 12.84%\nBatch 311/313 - Accuracy: 12.83%\nskipping last batch of size 16 --- gives shape error\nFinal Accuracy: 12.87%\nStudent (CSKD) accuracy on Super Pixelated = 12.87%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}